{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2017855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (13.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from faker) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from faker) (4.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from python-dateutil>=2.4->faker) (1.15.0)\n",
      "Requirement already satisfied: uszipcode==0.2.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.2.6)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from uszipcode==0.2.6) (20.3.0)\n",
      "Requirement already satisfied: pathlib-mate in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from uszipcode==0.2.6) (1.0.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from uszipcode==0.2.6) (2.26.0)\n",
      "Requirement already satisfied: SQLAlchemy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from uszipcode==0.2.6) (1.3.23)\n",
      "Requirement already satisfied: atomicwrites in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathlib-mate->uszipcode==0.2.6) (1.4.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathlib-mate->uszipcode==0.2.6) (1.15.0)\n",
      "Requirement already satisfied: autopep8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathlib-mate->uszipcode==0.2.6) (1.5.5)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode==0.2.6) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode==0.2.6) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode==0.2.6) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode==0.2.6) (1.26.8)\n",
      "Requirement already satisfied: pycodestyle>=2.6.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from autopep8->pathlib-mate->uszipcode==0.2.6) (2.6.0)\n",
      "Requirement already satisfied: toml in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from autopep8->pathlib-mate->uszipcode==0.2.6) (0.10.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install faker\n",
    "!pip install uszipcode==0.2.6\n",
    "\n",
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import csv\n",
    "import zipfile\n",
    "import scipy.stats\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from uszipcode import SearchEngine\n",
    "search = SearchEngine(simple_zipcode=False)\n",
    "fake = Faker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a434fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_lst = [\"Southeast Medical Center\", \"Marshall Medical Center\", \"Coffee Memorial Hospital\", \n",
    "           \"Mizell Memorial Hospital\", \"Crenshaw Community Hospital\", \"St Vincent's East\", \n",
    "           \"Dekalb Medical Center\",\"Baptist Medical Center\", \"Callahan Eye Hospital\", \n",
    "           \"Keller Memorial Hospital\", \"Dale Medical Center\", \"Cherokee Medical Center\", \"Medical Center South\",\n",
    "           \"Jackson Hospital & Clinic Inc\", \"East Medical Center\", \"Tanner Medical Center-east\", \n",
    "           \"University Of Harvard Hospital\", \"Community Hospital Inc\", \"Cullman Regional Medical Center\",\n",
    "           \"Andalusia Health\", \"Stringfellow Memorial Hospital\", \"Boston Hospital\",\n",
    "           \"Gadsden Regional Medical Center\",\"Marion Regional Medical Center\", \"New York Medical Center\",\n",
    "           \"Riverview Medical Center\", \"Georgian Medical Center\", \"Medical Center Enterprise\", \n",
    "           \"Greene County Hospital\", \"Lake Community Hospital\",\"Flowers Hospital\", \"St Vincent\", \n",
    "           \"San Burnadino Medical Center\", \"Lawrence Medical Center\", \"Highlands Medical Center\", \n",
    "           \"Wiregrass Medical Center\", \"Russell Medical Center\", \"Medical Center Barbour\", \"Clay Hospital\",\n",
    "           \"Northeast Regional Medical Center\", \"Limestone Hospital\", \"South Regional Medical Center\", \n",
    "           \"Decatur Morgan Hospital\", \"Northwest Medical Center\", \"University Of South Medical Center\", \n",
    "           \"Walker Medical Center\",\"Providence Hospital\", \"Grove Hill Memorial Hospital\",\n",
    "           \"Regional Medical Center\", \"Hale County Hospital\", \"Elmore Community Hospital\", \n",
    "           \"McMillan Memorial Hospital\", \"Thomas Hospital\", \"Citizens Medical Center\", \"Jones Hospital\",\n",
    "           \"Princeton Medical Center\", \"Grandview Medical Center\", \"Prattville Hospital\", \n",
    "           \"Pickens Medical Center\", \"Bullock Hospital\", \"Whitfield Memorial Hospital\", \n",
    "           \"Infirmary Medical Center\", \"Medical West\", \"Regional Medical Center Parkway Campus\", \n",
    "           \"Monroe Hospital\", \"Lakeland Community Hospital\", \"Troy Regional Medical Center\", \n",
    "           \"Jackson Medical Center\", \"North Baldwin Infirmary\", \"St Vincent's St Clair\", \n",
    "           \"Crestwood Medical Center\", \"Hill Hospital\", \"Brookwood Medical Center\", \"Springhill Memorial Hospital\",\n",
    "           \"Rmc Jacksonville\", \"Evergreen Medical Center\",\"Baptist Medical Center East\", \"Stabler Memorial Hospital\",\n",
    "           \"Shoals Hospital\", \"Russellville Hospital\",\"Coosa Valley Medical Center\", \"Arizona Medical Center\",\n",
    "           \"Jack Hughston Memorial Hospital\", \"Atmore Community Hospital\",\"St Vincent's Chilton\", \n",
    "           \"Washington County Hospital\", \"Red Bay Hospital\", \"Choctaw General Hospital\",\"St Vincents Blount\", \n",
    "           \"Children's Hospital\", \"University Children's And Women's Hospital\",\"Providence Medical Center\", \n",
    "           \"Mat-su Regional Medical Center\", \"Bartlett Regional Hospital\", \"Banner Medical Center\", \n",
    "           \"Fairbanks Memorial Hospital\", \"Alaska Hospital\", \"Yukon Reg Hospital\", \"Mountain Hospital\",\n",
    "           \"Pennsylvania General Hospital\", \"Alaska Medical Center\", \"Valdez Medical Center\", \"Seward Hospital\", \n",
    "           \"Community Hospital\", \"Petersburg Medical Center\", \"Wrangell Medical Center\", \"Peninsula Hospital\",\n",
    "           \"Providence Medical Ctr\", \"Cordova Community Medical Center\", \"Norton Regional Hospital\", \n",
    "           \"Sinai Hospital\", \"Milan Health Center\", \"Ketchikan Medical Center\", \"Samuel Memorial Hospital\"]\n",
    "\n",
    "null_lst = [\"LineConsideredAmount\", \"PreAuthNumber\", \"PriorClaimID\", \"PrimaryFlag\",  \"LineAdjustmentReasonCode\", \n",
    "            \"RelatedHospDischargeDate\", \"RelatedHospAdmitDate\", \"ClaimAdjustmentReasonCode\", \"NDC\", \n",
    "            \"LineAllowedAmount\", \"DenyReason\", \"ConditionCode\", \"ValueCode\", \"ServiceUnits\", \"LineUnitType\", \"CLIA\",\n",
    "            \"OccurrenceCode\",  \"DRGVersion\",  \"OtherPhysicianNPI\", \"AdmitSource\", \"AdmitType\", \"DischargeStatus\", \n",
    "            'OtherPhysicianNAME', \"EmergencyIndicator\", \"AdmittingDiag\", \"ICDProcDate1\", \"ICDProcDate2\", \n",
    "            \"ICDProcDate3\", \"ICDProcDate4\", \"ICDProc1\", \"ICDProc2\", \"ICDProc3\", \"ICDProc4\", \"HIPPSCode\",\n",
    "            \"ClaimStatusCategoryCode\", \"HCPC_Rate_HIPPSCode\", \"DRGCode\", \"RevenueCode\", \"TypeofBill\", \"ToothNumber\", \n",
    "            \"ToothBegin\", \"ToothEnd\", \"ToothSurfaces\", 'CertifyingPhysicianName', \"CertifyingPhysicianID\",\n",
    "            \"CertifyingPhysicianNPI\", \"CertifyingPhysicianNPI\", \"CertifyingPhysicianID\", \"OrderingProviderID\", \n",
    "            'OrderingProviderName', \"OrderingProviderID\", \"OrderingProviderNPI\", \"PrescribingProviderSpecialtyCode\",\n",
    "            'AttendingProviderNAME', \"AttendingProviderNPI\", \"PrescribingProviderID\", \"PrescribingProviderTIN\", \n",
    "            \"PrescribingProviderOrg\", 'PrescribingProviderFName', 'PrescribingProviderMName',\n",
    "            'PrescribingProviderLName', 'PrescribingProviderPhoneNumber', 'PrescribingProviderAddressLine1',\n",
    "            \"PrescribingProviderAddressLine2\", 'PrescribingProviderCity', \"PrescribingProviderNPI\", \n",
    "            'PrescribingProviderNPI', 'PrescribingProviderID', 'PrescribingProviderTIN',\n",
    "            \"PrescribingProviderTaxonomy\"]\n",
    "\n",
    "state_lst = ['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS',\n",
    "             'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', \n",
    "             'NY', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY']\n",
    "\n",
    "hcpc_lst = ['U0003', 'U0004', '87635', '87426', '90460', '90471', 'G2023', '0002A', 'U0005', '0001A', '86769', \n",
    "            '90461', 'C9803', '0202U', 'U0002', '0012A', '0011A', '87636', '90472', '0241U', '87428', '86328', \n",
    "            '87631', '87811', '87799', '0223U', '87637', '0031A', '86317', 'D0604', 'G2024', '87150', 'U0001', \n",
    "            '0240U', '99000', '86644', '87449', '87632', '0225U', '90474', '87634', '0100U', '99001', '0224U',\n",
    "            '0099U', '90473', '86318']\n",
    "default_pat = ('758 Nunez Stravenue@Apt. 340@8463@AK@1933-03-17@99547@Other@Robert@Sarah@Henderson@Spouse@Elderly'\n",
    "        '@758 Nunez Stravenue@Apt. 340@8321@AK@1933-11-30@99547@Male@Bryan@Victor@Henderson')\n",
    "\n",
    "ordered_col_lst = [\"ClaimID\", \"CapEpcIndicator\", \"SubscriberID\", \"SubscriberFName\",\"SubscriberMName\", \n",
    "                   \"SubscriberLName\", \"SubscriberDOB\", \"SubscriberGender\", \"SubscriberAddressLine1\",\n",
    "                   \"SubscriberAddressLine2\", \"SubscriberCity\", \"SubscriberState\", \"SubscriberZipCode\", \"PatientID\",\n",
    "                   \"PatientRelationship\", \"PatientFName\", \"PatientMName\", \"PatientLName\", \"PatientDOB\", \n",
    "                   \"PatientGender\",\"PatientAddressLine1\", \"PatientAddressLine2\", \"PatientCity\", \"PatientState\", \n",
    "                   \"PatientZipCode\",\"GroupNumber\", \"GroupName\", \"BillingProviderID\", \"BillingProviderNPI\", \n",
    "                   \"BillingProviderTIN\",\"BillingProviderSpecialtyCode\", \"BillingProviderTaxonomy\", \n",
    "                   \"BillingProviderOrg\", \"BillingProviderFName\",\"BillingProviderMName\", \"BillingProviderLName\", \n",
    "                   \"BillingProviderEntityType\", \"BillingProviderAddressLine1\",\"BillingProviderAddressLine2\", \n",
    "                   \"BillingProviderCity\", \"BillingProviderState\", \"BillingProviderZipCode\",\n",
    "                   \"BillingProviderPhoneNumber\", \"CertifyingPhysicianID\", \"CertifyingPhysicianName\", \n",
    "                   \"CertifyingPhysicianNPI\",\"OrderingProviderID\", \"OrderingProviderName\", \"OrderingProviderNPI\", \n",
    "                   \"ReferringProviderNPI\", \"InterPlan\",\"LineOfBusiness\", \"ClaimPaidDate\", \"ClaimReceivedDate\", \n",
    "                   \"ClaimServiceEndDate\", \"ClaimServiceStartDate\",\"ClaimAdjustmentReasonCode\", \"PriorClaimID\", \n",
    "                   \"ASOFlag\", \"PrimaryFlag\", \"ClaimStatusCategoryCode\",\"ClaimType\", \"ContractedFlag\", \"HdrCharges\", \n",
    "                   \"HdrCOBPay\", \"HdrPatientCopay\", \"HdrPatientPay\", \"HdrPay\",\"HdrPayerPay\", \"CLIA\",\n",
    "                   \"RelatedCausesCode\", \"AccidentDate\",\"EmergencyIndicator\", \"LineNumber\", \"NumofAdjustments\", \n",
    "                   \"LinePatientPaid\", \"LineAmountCoinsurance\",\"LineAmountCopay\", \"LineAmountDeductible\",\n",
    "                   \"LineCharges\", \"LinePayerPaid\", \"LineCOBPay\", \"LineTotPaid\",\"LineProcCode\", \"LineUnits\",\n",
    "                   \"LineUnitType\", \"LineConsideredAmount\", \"LineAllowedAmount\", \"ICDVersion\", \"LineServiceEndDate\",\n",
    "                   \"LineServiceStartDate\", \"LineAdjustmentReasonCode\", \"RenderingProviderID\",\"RenderingProviderNPI\",\n",
    "                   \"RenderingProviderTIN\", \"RenderingProviderSpecialtyCode\", \"RenderingProviderTaxonomy\",\n",
    "                   \"RenderingProviderOrg\", \"RenderingProviderFName\", \"RenderingProviderMName\", \n",
    "                   \"RenderingProviderLName\",\"RenderingProviderEntityType\", \"RenderingProviderAddressLine1\", \n",
    "                   \"RenderingProviderAddressLine2\",\"RenderingProviderCity\", \"RenderingProviderState\", \n",
    "                   \"RenderingProviderZipCode\", \"RenderingProviderPhoneNumber\",\"RelatedHospAdmitDate\",\n",
    "                   \"RelatedHospDischargeDate\", \"AdmitType\", \"AdmitSource\", \"DischargeStatus\", \"DRGCode\",\"DRGVersion\",\n",
    "                   \"AdmittingDiag\", \"ICDProc1\", \"ICDProc2\", \"ICDProc3\", \"ICDProc4\", \"ICDProcDate1\", \"ICDProcDate2\",\n",
    "                   \"ICDProcDate3\", \"ICDProcDate4\", \"APCCode\", \"CCN\", \"AttendingProviderNAME\", \"AttendingProviderNPI\",\n",
    "                   \"OtherPhysicianNAME\", \"OtherPhysicianNPI\", \"RevenueCode\", \"TypeofBill\", \"HCPC_Rate_HIPPSCode\",\n",
    "                   \"OccurrenceCode\",\"ServiceUnits\", \"PrescribingProviderID\", \"PrescribingProviderNPI\", \n",
    "                   \"PrescribingProviderTIN\",\"OutOfNetwork\",\"PreAuthIndicator\", \"PreAuthNumber\", \"NDC\", \"DenyFlag\",\n",
    "                   \"DenyReason\", \"HIPPSCode\", \"AdmitDate\", \"ToothNumber\",\"ToothBegin\", \"ToothEnd\", \"ToothSurfaces\",\n",
    "                   \"ValueCode\", \"ConditionCode\", \"PayeeType\"]\n",
    "\n",
    "diag_col_lst = [\"Diag1\", \"Diag2\", \"Diag3\", \"Diag4\", \"Diag5\", \"Diag6\",\"Diag7\", \"Diag8\", \"Diag9\", \"Diag10\", \"Diag11\",\n",
    "                \"Diag12\",\"Diag13\",\"Diag14\", \"Diag15\", \"Diag16\", \"Diag17\", \"Diag18\", \"Diag19\", \"Diag20\",\"Diag21\",\n",
    "                \"Diag22\",\"Diag23\",\"Diag24\",\"Diag25\"]\n",
    "\n",
    "ren_col_lst =[\"RenderingProviderEntityType\",\"RenderingProviderPhoneNumber\",\"RenderingProviderTIN\",\n",
    "              \"RenderingProviderID\",\"ContractedFlag\",\"OutOfNetwork\",\"RenderingProviderAddressLine1\",\n",
    "              \"RenderingProviderAddressLine2\",\"RenderingProviderOrg\",\"RenderingProviderFName\",\n",
    "              \"RenderingProviderMName\",\"RenderingProviderLName\",\"CCN\",\"RenderingProviderNPI\"]\n",
    "\n",
    "bill_col_lst = [\"BillingProviderID\", \"BillingProviderTIN\", \"BillingProviderOrg\", \"BillingProviderFName\", \n",
    "                \"BillingProviderMName\", \"BillingProviderLName\", \"BillingProviderPhoneNumber\", \n",
    "                \"BillingProviderAddressLine1\", \"BillingProviderAddressLine2\",\"BillingProviderEntityType\",\n",
    "                \"BillingProviderNPI\",'BillingProviderTaxonomy']\n",
    "\n",
    "pat_col_lst = ['PatientAddressLine1', 'PatientAddressLine2', 'PatientID', 'PatientState', 'PatientDOB', \n",
    "               'PatientZipCode', 'PatientGender', 'PatientFName', 'PatientMName', 'PatientLName', \n",
    "               'PatientRelationship', 'age_bin','SubscriberAddressLine1', 'SubscriberAddressLine2', 'SubscriberID', \n",
    "               'SubscriberState', 'SubscriberDOB', 'SubscriberZipCode', 'SubscriberGender', 'SubscriberFName', \n",
    "               'SubscriberMName', 'SubscriberLName']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509b5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tax = pd.read_csv(\"Input/Professional/codoxo_specialties_table.csv\")#switch to \"|\" deliniation for opening\n",
    "\n",
    "df_tax['id|specialty|taxonomy_codes'] = df_tax['id|specialty|taxonomy_codes'].str.split('|')\n",
    "\n",
    "def getLastValue(aList):\n",
    "    return aList[-1]\n",
    "\n",
    "df_tax['id|specialty|taxonomy_codes'] = df_tax['id|specialty|taxonomy_codes'].apply(getLastValue)\n",
    "df_tax['id|specialty|taxonomy_codes'] = df_tax['id|specialty|taxonomy_codes'].str.split('@')\n",
    "\n",
    "tax = df_tax.set_index('0')['id|specialty|taxonomy_codes'].to_dict()\n",
    "tax_ls = tax.values()\n",
    "tax_lst = [item for sublist in tax_ls for item in sublist]\n",
    "\n",
    "df_preauth = pd.read_csv(\"Input/Professional/preauth.csv\")\n",
    "df_preauth = df_preauth.sort_values(by=['percent'], ascending = False)\n",
    "df_preauth['percent'] = df_preauth['percent'].astype(float)\n",
    "df_preauth = df_preauth.drop(columns=[\"Unnamed: 0\"])\n",
    "preauth_dict = dict(df_preauth.values)\n",
    "\n",
    "nopay_df = pd.read_csv(\"Input/Professional/hdrpatpay.csv\")\n",
    "nopay_df = nopay_df.sort_values(by=['percent'], ascending = False)\n",
    "nopay_df = nopay_df.drop(columns=[\"Unnamed: 0\"])\n",
    "nopay_dict = dict(nopay_df.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9a26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcode(x):\n",
    "    return search.by_zipcode(x).city\n",
    "######################################################################################################################\n",
    "def nopay(x):\n",
    "    if x[\"LineProcCode\"] in nopay_dict.keys():\n",
    "        if random.randrange(100) < nopay_dict[x[\"LineProcCode\"]]*100:\n",
    "            return 0\n",
    "    return x[\"LinePayerPaid\"]\n",
    "######################################################################################################################\n",
    "def nopay2(x):\n",
    "    if x[\"LineProcCode\"] in nopay_dict.keys():\n",
    "        if random.randrange(100) < nopay_dict[x[\"LineProcCode\"]]*100:\n",
    "            return  np.random.choice([ \"Rejected\", \"Additional Information Requested\"], p=[0.7, 0.3,])\n",
    "    return \"Accepted\"\n",
    "######################################################################################################################\n",
    "def nopay3(x):\n",
    "    if x[\"LineProcCode\"] in nopay_dict.keys():\n",
    "            if random.randrange(100) < nopay_dict[x[\"LineProcCode\"]]*100:\n",
    "                return  \"Subsciber\"\n",
    "    return \"Prov\"\n",
    "######################################################################################################################\n",
    "def nopay4(x):\n",
    "    if x[\"ContractedFlag\"] == \"NONPAR\":\n",
    "        return  \"Subscriber\"\n",
    "    return x[\"PayeeType\"]\n",
    "######################################################################################################################\n",
    "def add_diag_values(pobjCombList, pobjWeights, pintLen):\n",
    "    objTmpOut = random.choices(pobjCombList, cum_weights = pobjWeights, k = pintLen)\n",
    "    return objTmpOut\n",
    "######################################################################################################################\n",
    "def preauth(x):\n",
    "    if x in preauth_dict.keys():\n",
    "        if random.randrange(100) <= preauth_dict[x]*100:\n",
    "            return \"Y\"\n",
    "    return \"N\"\n",
    "######################################################################################################################\n",
    "def in_network(x):\n",
    "    if x ==\"PAR\":\n",
    "        return \"N\"\n",
    "    return \"Y\"\n",
    "######################################################################################################################\n",
    "def ren_tax(rendering_df, taxonomy, num_providers):\n",
    "    rendering_df[\"random\"] = np.random.choice(['Y','N'], num_providers, p=[0.99, 0.01])\n",
    "    tax1_df = rendering_df.loc[(rendering_df['random'] == 'Y')]\n",
    "    tax2_df = rendering_df.loc[(rendering_df['random'] == 'N')]\n",
    "    tax1_df['RenderingProviderTaxonomy'] = np.random.choice(tax[taxonomy], tax1_df.shape[0])\n",
    "    tax2_df[\"RenderingProviderTaxonomy\"] = np.random.choice(tax_lst, tax2_df.shape[0])\n",
    "    rendering_df =pd.concat([tax1_df, tax2_df])\n",
    "    return rendering_df.drop(columns=['random'])\n",
    "######################################################################################################################\n",
    "def getting_final_combos(final_df, dfDiagCombos):\n",
    "    dfFinalCombos = pd.DataFrame()\n",
    "    dfCombosAdd = final_df.groupby(['combos', 'place_of_service', 'pat_gender', 'age_bin'], as_index=False).size()\n",
    "    dfCombosAdd = dfCombosAdd.merge(dfDiagCombos[['combos', 'place_of_service',\n",
    "                                                  'pat_gender', 'age_bin', 'diag_values', 'prob_diag']])\n",
    "    dfDV = dfCombosAdd.groupby(['combos', 'place_of_service',\n",
    "                                'pat_gender', 'age_bin', 'size'])['diag_values'].apply(list).reset_index()\n",
    "    dfPD = dfCombosAdd.groupby(['combos', 'place_of_service',\n",
    "                                'pat_gender', 'age_bin', 'size'])['prob_diag'].apply(list).reset_index()\n",
    "    dfCombosAdd = dfDV.merge(dfPD)\n",
    "    dfCombosAdd['diag_values_out'] = dfCombosAdd.apply(lambda x: add_diag_values(x.diag_values, x.prob_diag, x.size),\n",
    "                                                       axis=1)\n",
    "    return final_df.merge(dfCombosAdd)\n",
    "######################################################################################################################\n",
    "def def_bad_provs(final_df):\n",
    "    final_df[\"ren_id\"] = final_df[\"ren_id\"].astype(int)\n",
    "    sb_df= final_df.loc[(final_df['ren_id'] <=3)]\n",
    "    nb_df = final_df.loc[(final_df['ren_id'] > 3)]\n",
    "    sb_df[\"super_bad\"] = \"Y\"\n",
    "    sb_df[\"bad1\"] = \"N\"\n",
    "    sb_df[\"bad2\"] = \"N\"\n",
    "    nb_df[\"super_bad\"] = \"N\"\n",
    "    nb_df[\"bad1\"] = \"N\"\n",
    "    nb_df[\"bad2\"] =  \"N\"\n",
    "    nb_df[\"bad3\"] =  \"N\"\n",
    "    nb_df.iloc[75:180, nb_df.columns.get_loc('bad1')] = \"Y\"\n",
    "    nb_df.iloc[172:265, nb_df.columns.get_loc('bad2')] = \"Y\"\n",
    "    nb_df.iloc[5:75, nb_df.columns.get_loc('bad3')] = \"Y\"\n",
    "\n",
    "    final_df = pd.concat([sb_df, nb_df])\n",
    "    return final_df.drop(columns=[\"prob_diag\",\"diag_values_out\"]) \n",
    "######################################################################################################################\n",
    "def know_your_mohs(final_df, all_combo_csv, folder, c_amt_csv, d_amt_csv):\n",
    "    if folder ==\"Pathology\": #AI Alert know your Mohs.\n",
    "        ## Unique combinations and corresponding weights\n",
    "        #14001|14021|14041|14061|14301|14302\n",
    "        all_combinations = pd.read_csv(all_combo_csv)\n",
    "        all_combinations[\"prob_claims\"] = all_combinations[\"prob_claims\"].astype(float)\n",
    "        all_combinations[\"prob_claims\"] = all_combinations[\"prob_claims\"] * all_combinations[\"prob_claims\"]\n",
    "        all_combinations= all_combinations[all_combinations[\"combos\"].str.contains(\"INT\")==False]\n",
    "        all_combinations= all_combinations[all_combinations[\"combos\"].str.contains(\"1P\")==False]\n",
    "        all_combinations= all_combinations[all_combinations[\"combos\"].str.contains(\"09036\")==False]\n",
    "        \n",
    "        sel_columns1= all_combinations[all_combinations[\"combos\"].str.contains(\"14001\")]\n",
    "        sel_columns2= all_combinations[all_combinations[\"combos\"].str.contains(\"14021\")]\n",
    "        sel_columns3= all_combinations[all_combinations[\"combos\"].str.contains(\"14041\")]\n",
    "        sel_columns4= all_combinations[all_combinations[\"combos\"].str.contains(\"14061\")]\n",
    "        sel_columns5= all_combinations[all_combinations[\"combos\"].str.contains(\"14301\")]\n",
    "        sel_columns6= all_combinations[all_combinations[\"combos\"].str.contains(\"14302\")]\n",
    "        \n",
    "        all_combinations=pd.concat([sel_columns1,sel_columns2,sel_columns3, sel_columns4, sel_columns5, sel_columns6])\n",
    "        t1_df= pd.read_csv(c_amt_csv)\n",
    "        t2_df= pd.read_csv(d_amt_csv)\n",
    "        \n",
    "        t1_lst = t1_df['combo'].tolist()\n",
    "        t2_lst = t2_df['combo'].tolist()\n",
    "        t3_lst = t1_lst and t2_lst\n",
    "        \n",
    "        all_combinations[\"combos1\"] = all_combinations[\"combos\"].apply(lambda x: x.split(\":\"))\n",
    "        safe_lst = []\n",
    "        a = \"\"\n",
    "        all_combinations['combos1'] = all_combinations['combos'].str.replace(';', '@')\n",
    "        all_combinations[\"combos1\"] = all_combinations[\"combos1\"].apply(lambda x: x.split(\":\"))\n",
    "        all_combinations['test1'] = all_combinations['combos1'].apply(lambda x: Counter(x))\n",
    "        test2 = Counter(t3_lst)\n",
    "        all_combinations['safe'] = all_combinations['test1'].apply(lambda x: True if sum(x.values()) == \n",
    "                                                                   sum((x & test2).values()) else False)    \n",
    "\n",
    "        sel_combinations = all_combinations.loc[all_combinations['safe'] == True]\n",
    "        sel_combinations =sel_combinations.drop(columns=[\"combos1\",\"safe\"])\n",
    "        ## Unique combinations and corresponding weights\n",
    "        comb_list = sel_combinations.drop(['num_claims', 'prob_claims'], axis = 1).values\n",
    "        weights = sel_combinations['prob_claims'].tolist()\n",
    "        sel_columns= sel_combinations.drop(['num_claims', 'prob_claims'], axis = 1).columns\n",
    "        cdf = [weights[0]]\n",
    "        for w in weights[1:]:\n",
    "            cdf.append(cdf[-1]+w)\n",
    "       \n",
    "        final_df_list = []\n",
    "        random_outliers = random.sample(range(0, num_providers), int(round(0.01*num_providers)))\n",
    "        for ii in range(0, num_providers//100):\n",
    "            \n",
    "            if ii in random_outliers:\n",
    "                tmp = pd.DataFrame(random.choices(comb_list, k=group[ii]//10), columns = sel_columns)\n",
    "            else:\n",
    "                tmp = pd.DataFrame(random.choices(comb_list, cum_weights = cdf, k=group[ii]//10), columns =\n",
    "                                   sel_columns)\n",
    "            tmp['ren_id'] = str(ii+1)\n",
    "            final_df_list.append(tmp)\n",
    "        final_df1 = pd.concat(final_df_list)\n",
    "        final_df = pd.concat([final_df1, final_df])\n",
    "    return final_df\n",
    "######################################################################################################################\n",
    "def covid_drive_thru(final_df):\n",
    "    th_df1= final_df.loc[(final_df['super_bad'] == 'Y')]\n",
    "    th_df1 = th_df1.sample(frac=1)\n",
    "    th_df1 = th_df1.iloc[0:th_df1.shape[0]//4]\n",
    "    th_df1 = th_df1[th_df1[\"combos\"].str.contains(\"9921\")]\n",
    "    th_df2 = th_df1[th_df1[\"combos\"].str.contains(\"9920\")]\n",
    "    dt_df = final_df[final_df[\"combos\"].str.contains(\"9921\")]\n",
    "    dt_df1 = final_df[final_df[\"combos\"].str.contains(\"9920\")]\n",
    "    dt_df= dt_df.loc[(dt_df['bad1'] == 'Y')]\n",
    "    dt_df1= dt_df1.loc[(dt_df1['bad1'] == 'Y')]\n",
    "    dt_df = pd.concat([th_df1,dt_df1,th_df2, dt_df])\n",
    "    dt_df[\"drop\"] = np.random.choice(hcpc_lst, dt_df.shape[0])\n",
    "    dt_df[\"combos\"] = dt_df[\"combos\"]+\":\"+dt_df[\"drop\"]\n",
    "    dt_df= dt_df.drop(columns=[\"drop\"])\n",
    "    return  pd.concat([final_df, dt_df])\n",
    "######################################################################################################################\n",
    "def state_info(rendering_df, location_csv):\n",
    "    zip_length = rendering_df.shape[0] + 10\n",
    "    zip_df =pd.read_csv(location_csv)\n",
    "    zip_df['subscriberstate'] = zip_df['subscriberstate'].astype(str)\n",
    "    zip_df[\"billingproviderstate\"]=zip_df[\"billingproviderstate\"].astype(str)\n",
    "    zip_df[\"renderingproviderstate\"]=zip_df[\"renderingproviderstate\"].astype(str)\n",
    "    zip_df[\"prescribingproviderstate\"]=zip_df[\"prescribingproviderstate\"].astype(str)\n",
    "    zip_df1= zip_df.loc[(zip_df['subscriberstate'] == \"0.0\")]\n",
    "    zip_df2= zip_df.loc[(zip_df['subscriberstate'] != \"0.0\")]\n",
    "    zip_df1[\"subscriberstate\"] = \"GA\"\n",
    "    zip_df = pd.concat([zip_df1, zip_df2])\n",
    "    \n",
    "    zip_df1= zip_df.loc[(zip_df['billingproviderstate'] == \"0.0\")]\n",
    "    zip_df2= zip_df.loc[(zip_df['billingproviderstate'] != \"0.0\")]\n",
    "    zip_df1[\"billingproviderstate\"] = zip_df1[\"subscriberstate\"] \n",
    "    zip_df = pd.concat([zip_df1, zip_df2])\n",
    "    \n",
    "    zip_df1= zip_df.loc[(zip_df['renderingproviderstate'] == \"0.0\")]\n",
    "    zip_df2= zip_df.loc[(zip_df['renderingproviderstate'] != \"0.0\")]\n",
    "    zip_df1[\"renderingproviderstate\"] = zip_df1[\"billingproviderstate\"] \n",
    "    zip_df = pd.concat([zip_df1, zip_df2])\n",
    "    \n",
    "    zip_df1= zip_df.loc[(zip_df['prescribingproviderstate'] == \"0.0\")]\n",
    "    zip_df2= zip_df.loc[(zip_df['prescribingproviderstate'] != \"0.0\")]\n",
    "    zip_df1[\"prescribingproviderstate\"] = zip_df1[\"renderingproviderstate\"] \n",
    "    zip_df = pd.concat([zip_df1, zip_df2])\n",
    "\n",
    "    zip_df[\"prob_combo\"] = zip_df[\"prob_combo\"].astype(float)\n",
    "    zip_df[\"prob_combo\"] = 3*zip_df[\"prob_combo\"]*zip_df[\"prob_combo\"]\n",
    "    return zip_df.set_index(['billingproviderstate', 'renderingproviderstate','subscriberstate', \n",
    "                               'prescribingproviderstate', 'combo_tot', \n",
    "                               'prob_combo']).astype('float64').astype('int64').reset_index()    \n",
    "######################################################################################################################\n",
    "def find_zips(dfRendChoices):\n",
    "    dfRendChoices['subscriberzipcode'] = dfRendChoices['subscriberzipcode'].astype(str)\n",
    "    dfRendChoices['renderingproviderzipcode'] =dfRendChoices['renderingproviderzipcode'].astype(str)\n",
    "    dfRendChoices['patientzipcode']=dfRendChoices['patientzipcode'].astype(str)\n",
    "    dfRendChoices['billingproviderzipcode']=dfRendChoices['billingproviderzipcode'].astype(str)\n",
    "    \n",
    "    dfRendChoices['SubscriberCity'] = dfRendChoices['subscriberzipcode'].str[:5]\n",
    "    dfRendChoices['RenderingProviderCity'] = dfRendChoices['renderingproviderzipcode'].str[:5]\n",
    "    dfRendChoices['PatientCity'] = dfRendChoices['patientzipcode'].str[:5]\n",
    "    dfRendChoices['BillingProviderCity'] = dfRendChoices['billingproviderzipcode'].str[:5]\n",
    "    \n",
    "    dfRendChoices['BillingProviderCity'] = dfRendChoices['BillingProviderCity'].str.replace('.','')\n",
    "    dfRendChoices['PatientCity'] = dfRendChoices['PatientCity'].str.replace('.','')\n",
    "    dfRendChoices['SubscriberCity'] =dfRendChoices['SubscriberCity'].str.replace('.','')\n",
    "    dfRendChoices['RenderingProviderCity'] = dfRendChoices['RenderingProviderCity'].str.replace('.','')\n",
    "    \n",
    "    dfRendChoices['BillingProviderCity'] = dfRendChoices['BillingProviderCity'].astype(float)\n",
    "    dfRendChoices['PatientCity'] = dfRendChoices['PatientCity'].astype(float)\n",
    "    dfRendChoices['SubscriberCity'] = dfRendChoices['SubscriberCity'].astype(float)\n",
    "    dfRendChoices['RenderingProviderCity'] = dfRendChoices['RenderingProviderCity'].astype(float)\n",
    "    \n",
    "    dfRendChoices['SubscriberCity'] = dfRendChoices['SubscriberCity'].fillna(0).astype(int).apply(zcode)\n",
    "    dfRendChoices['RenderingProviderCity'] = dfRendChoices['RenderingProviderCity'].fillna(0).astype(int).apply(zcode)\n",
    "    dfRendChoices['PatientCity'] = dfRendChoices['PatientCity'].fillna(0).astype(int).apply(zcode)\n",
    "    dfRendChoices['BillingProviderCity'] = dfRendChoices['BillingProviderCity'].fillna(0).astype(int).apply(zcode)\n",
    "    dfRendChoices.rename(columns = {'billingproviderzipcode':'BillingProviderZipCode',\n",
    "                                    \"renderingproviderzipcode\": \"RenderingProviderZipCode\",\n",
    "                                    \"pickupzipcode\":\"PickUpZipCode\", \"dropoffzipcode\":\"DropOffZipCode\",\n",
    "                                    \"prescribingproviderzipcode\":\"PrescribingProviderZipCode\",\n",
    "                                    \"subscriberzipcode\":\"SubscriberZipCode\", \"patientzipcode\":\"PatientZipCode\",\n",
    "                                    \"billingproviderstate\":\"BillingProviderState\", \n",
    "                                    'renderingproviderstate':\"RenderingProviderState\",\n",
    "                                    'subscriberstate':\"SubscriberState\",\n",
    "                                    'prescribingproviderstate':\"PrescribingProviderState\"}, inplace = True)    \n",
    "    return dfRendChoices\n",
    "######################################################################################################################\n",
    "def bill_info(num_providers, taxonomy, billing_prov_dict):\n",
    "    billing_df = pd.DataFrame()\n",
    "    billing_df['BillingProviderTaxonomy'] = np.random.choice(tax[taxonomy], num_providers//6)\n",
    "    billing_df[\"bill\"] = np.random.choice(list(billing_prov_dict.keys()),billing_df.shape[0],\n",
    "                                          p=list(billing_prov_dict.values()))\n",
    "    billing_df[\"Billing\"] = billing_df[\"bill\"] +\"@\"+ billing_df['BillingProviderTaxonomy']\n",
    "    return billing_df[\"Billing\"].tolist()\n",
    "######################################################################################################################\n",
    "def group_info(num_providers):\n",
    "    group_df = pd.DataFrame()\n",
    "    group_df['GroupName'] = [fake.company() for i in range(num_providers)]\n",
    "    group_df['GroupNumber']  = np.random.randint(10000,999999, size=num_providers)\n",
    "    group_df['GroupNumber'] = group_df['GroupNumber'].astype(str)\n",
    "    group_df[\"Group\"] = group_df['GroupName'] +\"@\"+ group_df['GroupNumber']\n",
    "    return group_df[\"Group\"].tolist()\n",
    "######################################################################################################################\n",
    "def claim_date(final_df):\n",
    "    two_y_ago = datetime.now().date() - relativedelta(years=2)\n",
    "    final_df[\"AccidentDate\"] = np.random.choice(pd.date_range(two_y_ago, datetime.now()- relativedelta(months=1)),\n",
    "                                                len(final_df))\n",
    "    delta = pd.to_timedelta(np.random.randint(0,4, size=len(final_df)), unit='d')\n",
    "    final_df[\"AdmitDate\"] = final_df[\"AccidentDate\"]+delta\n",
    "    delta = pd.to_timedelta(np.random.randint(0,3, size=len(final_df)), unit='d')\n",
    "    final_df[\"ClaimServiceStartDate\"] = final_df[\"AdmitDate\"] +delta\n",
    "    delta = pd.to_timedelta(np.random.randint(0,3, size=len(final_df)), unit='d')\n",
    "    final_df[\"LineServiceStartDate\"] = final_df[\"ClaimServiceStartDate\"]\n",
    "    final_df[\"LineServiceEndDate\"]=final_df[\"ClaimServiceStartDate\"]\n",
    "    final_df[\"ClaimServiceEndDate\"]=final_df[\"ClaimServiceStartDate\"]+delta \n",
    "    delta = pd.to_timedelta(np.random.randint(0,3, size=len(final_df)), unit='d')\n",
    "    final_df[\"ClaimReceivedDate\"] = final_df[\"ClaimServiceEndDate\"]+delta\n",
    "    delta = pd.to_timedelta(np.random.randint(0,3, size=len(final_df)), unit='d')\n",
    "    final_df[\"ClaimPaidDate\"] = final_df[\"ClaimReceivedDate\"] +delta\n",
    "    #remove accident date for all but 2% of claims to mimick real data. \n",
    "    #Probably should eventually be based on hcpcs and mod but not now.\n",
    "    number_one = len(final_df) // 50    \n",
    "    final_df_date = final_df.iloc[:number_one]\n",
    "    final_df_ndate = final_df.iloc[number_one:]\n",
    "    final_df_date[\"AccidentDate\"]=final_df_date[\"AccidentDate\"].astype(str)\n",
    "    final_df_date[\"RelatedCausesCode\"] = np.random.choice([\"AA\", \"AP\",\"EM\", \"OA\"], final_df_date.shape[0],\n",
    "                                                          p=[0.2, 0.1,0.3, 0.4])\n",
    "    final_df_date[\"LineCOBPay\"] = np.random.uniform(.7,.9, size = final_df_date.shape[0])\n",
    "    \n",
    "    final_df_ndate[\"AccidentDate\"]=\"\"\n",
    "    final_df_ndate[\"LineCOBPay\"] = 0\n",
    "    final_df_date[\"RelatedCausesCode\"] =\"\"\n",
    "    return pd.concat([final_df_ndate, final_df_date])\n",
    "######################################################################################################################\n",
    "def prov_zips(final_df):\n",
    "    final_df[\"BillingProviderZipCode\"]= final_df[\"BillingProviderZipCode\"].astype(str)\n",
    "    final_df[\"RenderingProviderZipCode\"]=  final_df[\"RenderingProviderZipCode\"].astype(str)\n",
    "    final_df[\"PrescribingProviderZipCode\"]= final_df[\"PrescribingProviderZipCode\"].astype(str)\n",
    "    final_df[\"BillingProviderState\"] = final_df[\"BillingProviderState\"].astype(str)\n",
    "    final_df[\"RenderingProviderState\"]= final_df[\"RenderingProviderState\"].astype(str)\n",
    "    final_df[\"PrescribingProviderState\"]= final_df[\"PrescribingProviderState\"].astype(str)\n",
    "    final_df[\"BillingProviderZipCode\"]= final_df[\"BillingProviderZipCode\"].replace(['0'], np.nan, regex=True, \n",
    "                                                                                   inplace=True)\n",
    "    final_df[\"RenderingProviderZipCode\"]=  final_df[\"RenderingProviderZipCode\"].replace(['0'], np.nan, regex=True, \n",
    "                                                                                        inplace=True)\n",
    "    final_df[\"PrescribingProviderZipCode\"]= final_df[\"PrescribingProviderZipCode\"].replace(['0'], np.nan,\n",
    "                                                                                           regex=True, inplace=True)\n",
    "    final_df[\"BillingProviderState\"] = final_df[\"BillingProviderState\"].str.replace('0.0','')\n",
    "    final_df[\"RenderingProviderState\"]= final_df[\"RenderingProviderState\"].str.replace('0.0','')\n",
    "    final_df[\"PrescribingProviderState\"]= final_df[\"PrescribingProviderState\"].str.replace('0.0','')\n",
    "    final_df[\"SubscriberState\"] = final_df[\"SubscriberState\"].astype(str)\n",
    "    return final_df\n",
    "######################################################################################################################\n",
    "def pat_states(final_df):\n",
    "    dfs = []\n",
    "    final_out = pd.DataFrame()\n",
    "    for st in state_lst: \n",
    "        final_df_st = final_df\n",
    "        strOutFile = 's3://{}/{}'.format('data-science-synthdata-dev', \"Patients/\"+st+\".csv\")\n",
    "        pat_df = pd.read_csv(strOutFile)\n",
    "        \n",
    "        pat_df[\"PatientID\"] = pat_df[\"PatientID\"].astype(str)\n",
    "        pat_df[\"SubscriberID\"] = pat_df[\"SubscriberID\"].astype(str)\n",
    "        pat_df[\"PatientZipCode\"] = pat_df[\"PatientZipCode\"].astype(str)\n",
    "        pat_df[\"SubscriberZipCode\"] = pat_df[\"SubscriberZipCode\"].astype(str)\n",
    "        pat_df[\"Pat\"] = (pat_df['PatientAddressLine1'] +'@'+pat_df['PatientAddressLine2'] +'@'+pat_df['PatientID'] +'@'+pat_df['PatientState'] +'@'+pat_df['PatientDOB'] +'@'+pat_df['PatientZipCode'] +'@'+pat_df['PatientGender'] +'@'+pat_df['PatientFName'] +'@'+pat_df['PatientMName']+'@'+pat_df['PatientLName'] +'@'+pat_df['PatientRelationship'] +'@'+pat_df['age_bin']+'@'+pat_df['SubscriberAddressLine1'] +'@'+pat_df['SubscriberAddressLine2'] +'@'+pat_df['SubscriberID']+'@'+pat_df['SubscriberState'] +'@'+pat_df['SubscriberDOB'] +'@'+pat_df['SubscriberZipCode']+'@'+pat_df['SubscriberGender'] +'@'+pat_df['SubscriberFName'] +'@'+pat_df['SubscriberMName'])\n",
    "        pat_df['SubscriberLName']\n",
    "        pat_df[\"Pat\"] = pat_df[\"Pat\"].astype(str)\n",
    "        adult_df = pat_df.loc[(pat_df['age_bin'] == 'Adult')]\n",
    "        adult_df_m = adult_df.loc[(adult_df['PatientGender'] == 'Male')]\n",
    "        adult_df_f = adult_df.loc[(adult_df['PatientGender'] == 'Female')]\n",
    "        adult_df_o = adult_df.loc[(adult_df['PatientGender'] == 'Other')]\n",
    "        \n",
    "        elderly_df = pat_df.loc[(pat_df['age_bin'] == 'Elderly')]    \n",
    "        elderly_df_m = elderly_df.loc[(elderly_df['PatientGender'] == 'Male')]\n",
    "        elderly_df_f = elderly_df.loc[(elderly_df['PatientGender'] == 'Female')]\n",
    "        elderly_df_o = elderly_df.loc[(elderly_df['PatientGender'] == 'Other')]\n",
    "\n",
    "        minor_df = pat_df.loc[(pat_df['age_bin'] == 'Minor')]\n",
    "        minor_df_m = minor_df.loc[(minor_df['PatientGender'] == 'Male')]\n",
    "        minor_df_f = minor_df.loc[(minor_df['PatientGender'] == 'Female')]\n",
    "        minor_df_o = minor_df.loc[(minor_df['PatientGender'] == 'Other')]\n",
    "\n",
    "        adult_list_m = adult_df_m[\"Pat\"].tolist()\n",
    "        adult_list_f = adult_df_f[\"Pat\"].tolist()\n",
    "        adult_list_o = adult_df_o[\"Pat\"].tolist()\n",
    "\n",
    "        elderly_list_m = elderly_df_m[\"Pat\"].tolist()\n",
    "        elderly_list_f = elderly_df_f[\"Pat\"].tolist()\n",
    "        elderly_list_o = elderly_df_o[\"Pat\"].tolist()\n",
    "\n",
    "        minor_list_m = minor_df_m[\"Pat\"].tolist()\n",
    "        minor_list_f = minor_df_f[\"Pat\"].tolist()\n",
    "        minor_list_o = minor_df_o[\"Pat\"].tolist()\n",
    "        \n",
    "        \n",
    "        minor_list_m= [x for x in minor_list_m if x != \"nan\"]\n",
    "        minor_list_f= [x for x in minor_list_f if x != \"nan\"]\n",
    "        minor_list_o= [x for x in minor_list_o if x != \"nan\"]\n",
    "        \n",
    "        adult_list_m= [x for x in adult_list_m if x != \"nan\"]\n",
    "        adult_list_f= [x for x in adult_list_f if x != \"nan\"]\n",
    "        adult_list_o= [x for x in adult_list_o if x != \"nan\"]\n",
    "  \n",
    "        elderly_list_m= [x for x in elderly_list_m if x != \"nan\"]\n",
    "        elderly_list_f= [x for x in elderly_list_f if x != \"nan\"]\n",
    "        elderly_list_o= [x for x in elderly_list_o if x != \"nan\"]\n",
    "        \n",
    "        minor_list_m.append(default_pat)\n",
    "        minor_list_f.append(default_pat)\n",
    "        minor_list_o.append(default_pat)\n",
    "        adult_list_m.append(default_pat)\n",
    "        adult_list_f.append(default_pat)\n",
    "        adult_list_o.append(default_pat)\n",
    "        elderly_list_m.append(default_pat)\n",
    "        elderly_list_f.append(default_pat)\n",
    "        elderly_list_o.append(default_pat)\n",
    "        \n",
    "        final_df_st = final_df.loc[(final_df['SubscriberState'] == st)]\n",
    "        if st == \"GA\":\n",
    "            final_df_null = final_df.loc[(final_df['SubscriberState'] == \"0.0\")]\n",
    "            final_df_st = final_df_st.append(final_df_null)\n",
    "        final_df_st = final_df_st.reset_index(drop = True)\n",
    "        if final_df_st.shape[0]==0:\n",
    "            continue\n",
    "        adult_df = final_df_st[(final_df_st['age_bin'] == 'Adult')]\n",
    "        adult_df_m = adult_df[(adult_df['pat_gender'] == 'M')]\n",
    "        adult_df_f = adult_df.loc[(adult_df['pat_gender'] == 'F')]\n",
    "        adult_df_o = adult_df.loc[(adult_df['pat_gender'] == 'None')]\n",
    "        \n",
    "        adult_df_m[\"Pat\"] = np.random.choice(adult_list_m, size=adult_df_m.shape[0])\n",
    "        adult_df_f[\"Pat\"] = np.random.choice(adult_list_f, size=adult_df_f.shape[0])\n",
    "        adult_df_o[\"Pat\"] = np.random.choice(adult_list_o, size=adult_df_o.shape[0])\n",
    "        \n",
    "        elderly_df = final_df_st.loc[(final_df_st['age_bin'] == 'Elderly')]\n",
    "        elderly_df_m = elderly_df.loc[(elderly_df['pat_gender'] == 'M')]\n",
    "        elderly_df_f = elderly_df.loc[(elderly_df['pat_gender'] == 'F')]\n",
    "        elderly_df_o = elderly_df.loc[(elderly_df['pat_gender'] == 'None')]\n",
    "        \n",
    "        elderly_df_m[\"Pat\"] = np.random.choice(elderly_list_m, size=elderly_df_m.shape[0])\n",
    "        elderly_df_f[\"Pat\"] = np.random.choice(elderly_list_f, size=elderly_df_f.shape[0])\n",
    "        elderly_df_o[\"Pat\"] = np.random.choice(elderly_list_o, size=elderly_df_o.shape[0])\n",
    "        \n",
    "        minor_df = final_df_st.loc[(final_df_st['age_bin'] == 'Minor')]\n",
    "        minor_df_m = minor_df.loc[(minor_df['pat_gender'] == 'M')]\n",
    "        minor_df_f = minor_df.loc[(minor_df['pat_gender'] == 'F')]\n",
    "        minor_df_o = minor_df.loc[(minor_df['pat_gender'] == 'None')]\n",
    "\n",
    "        minor_df_m[\"Pat\"] = np.random.choice(minor_list_m, size=minor_df_m.shape[0])\n",
    "        minor_df_f[\"Pat\"] = np.random.choice(minor_list_f, size=minor_df_f.shape[0])\n",
    "        minor_df_o[\"Pat\"] = np.random.choice(minor_list_o, size=minor_df_o.shape[0])\n",
    "        final_df_st = pd.concat([minor_df_m, minor_df_f, minor_df_o, elderly_df_m, elderly_df_f, elderly_df_o,\n",
    "                                 adult_df_f,adult_df_m, adult_df_o])\n",
    "        \n",
    "        final_out = final_out.append(final_df_st)\n",
    "    return final_out\n",
    "######################################################################################################################\n",
    "def null_bill_zips(final_df):\n",
    "    nz_df= final_df.loc[(final_df['BillingProviderZipCode'] == \"\")]\n",
    "    z_df=final_df.loc[(final_df['BillingProviderZipCode'] != \"\")]\n",
    "    nz_df[\"BillingProviderAddressLine1\"] =\"\"\n",
    "    nz_df[\"BillingProviderAddressLine2\"] =\"\" \n",
    "    return pd.concat([nz_df, z_df])\n",
    "######################################################################################################################\n",
    "def bad_prov_lst(final_df, folder):\n",
    "    bad_df = final_df.loc[(final_df['super_bad'] == 'Y')]\n",
    "    sb_out_lst = bad_df['RenderingProviderNPI'].tolist()\n",
    "    sb_out_lst = list(set(sb_out_lst))\n",
    "    with open(\"Out/Professional/\"+folder+' bad_prov.txt', 'w') as f:\n",
    "        for item in sb_out_lst:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "######################################################################################################################\n",
    "def insurance_type(final_df):\n",
    "    e_df = final_df.loc[(final_df['age_bin'] == 'Elderly')]\n",
    "    n_df = final_df.loc[(final_df['age_bin'] != 'Elderly')]    \n",
    "    e_df[\"LineOfBusiness\"] = np.random.choice([\"Medicare\", \"Commercial Plus\"],  e_df.shape[0], p=[0.95, 0.05])\n",
    "    n_df[\"LineOfBusiness\"] = np.random.choice([\"Commercial\", \"Medicare\", \"Commercial Plus\"], n_df.shape[0], \n",
    "                                              p=[0.5, 0.05, 0.45])\n",
    "    return pd.concat([n_df, e_df])    \n",
    "######################################################################################################################\n",
    "def claim_peppering(final_df):\n",
    "    #time decrease\n",
    "    i = 0\n",
    "    j = 0\n",
    "    w = 0\n",
    "    final_df = final_df.reset_index()\n",
    "    while w < 15:\n",
    "        w+=1\n",
    "        c= final_df.shape[0] - 40\n",
    "        i = np.random.randint(10, c)\n",
    "        j = i+np.random.randint(5, 20)\n",
    "        final_df = final_df.drop(final_df.index[i:j])\n",
    "\n",
    "    \n",
    "    ##time increase\n",
    "    \n",
    "    #i = 0\n",
    "    #j = 0\n",
    "    #w = 0\n",
    "    #final_df=final_df.sort_values(by=['combos', 'AdmitDate',\"RenderingProviderNPI\"])\n",
    "    #while w < 20:\n",
    "    #    w+=1\n",
    "    #    c= final_df.shape[0] - 40\n",
    "    #    i = np.random.randint(10, c)\n",
    "    #    j = i+np.random.randint(5, 20)\n",
    "    #    split df in 2 one has range other doesn't chage date to decrease num months concat back together\n",
    "    \n",
    "    #claim duplicates\n",
    "    i = 0\n",
    "    j = 0\n",
    "    w = 0\n",
    "    final_df = final_df.reset_index()\n",
    "    \n",
    "    while w < 80:\n",
    "        w+=1\n",
    "        i = np.random.randint(50,c)\n",
    "        j = i+np.random.randint(10, 20)\n",
    "        df = final_df.sample(frac=0.01, replace=True, random_state=1)\n",
    "        final_df = pd.concat([final_df, df])\n",
    "    sb_df1= final_df.loc[(final_df['super_bad'] == 'Y')]\n",
    "    df1 = pd.DataFrame()\n",
    "    while w < 10:\n",
    "        w+=1\n",
    "        try:\n",
    "            c= sb_df1.shape[0] - 10\n",
    "            i = np.random.randint(10,c)\n",
    "            j = i+np.random.randint(10, 20)\n",
    "            df = sb_df1.sample(frac=0.01, replace=True, random_state=1)\n",
    "            df1 = pd.concat([df1, df])\n",
    "        except:\n",
    "            print(\"dupes failed for a run\")\n",
    "    return pd.concat([final_df, df1])\n",
    "######################################################################################################################\n",
    "def line_nums(final_df):\n",
    "    line_df = pd.DataFrame()\n",
    "    line_df[\"len\"] = final_df['combos'].str.len()\n",
    "    line_df['LineNumber'] = line_df.apply(lambda x: list(range(1, x.len + 1)), axis=1)\n",
    "    line_df= line_df.explode('LineNumber')\n",
    "    \n",
    "    final_df = final_df.explode('combos')#spliting up line into HCPCS and mod value in list, first is Hcpcs, \n",
    "    #second is mod for line\n",
    "    final_df[\"LineNumber\"]=line_df[\"LineNumber\"]\n",
    "    del line_df\n",
    "    final_df[\"combos\"] = final_df[\"combos\"].apply(lambda x: x.split(\";\"))\n",
    "    final_df[['hcpcs_code','mod_values']] = pd.DataFrame(final_df.pop('combos').values.tolist(), index=final_df.index)\n",
    "    final_df['hcpcs_code'] = final_df['hcpcs_code'].apply(lambda x: '{0:0>5}'.format(x))\n",
    "    final_df['ren_id'] = final_df['ren_id'].astype(str)\n",
    "    final_df[\"combos\"] = final_df[\"hcpcs_code\"] +\";\"+final_df[\"mod_values\"]\n",
    "    final_df[\"combos\"] =final_df[\"combos\"].fillna(\"\")\n",
    "    return final_df\n",
    "######################################################################################################################\n",
    "def telehealth_fraud(final_df, c_amt_csv, d_amt_csv):\n",
    "    th_df= final_df.loc[(final_df['super_bad'] == 'Y')]\n",
    "    th_df = th_df.sample(frac=1)\n",
    "    th_df = th_df.iloc[0:th_df.shape[0]//4]\n",
    "    th_df = th_df[th_df[\"combos\"].str.contains(\"76805\")]\n",
    "    dt_df = final_df[final_df[\"combos\"].str.contains(\"76805\")]\n",
    "\n",
    "    th1_df = th_df.loc[(th_df['mod_values'] == '')]\n",
    "    th2_df = th_df.loc[(th_df['mod_values'] != '')]\n",
    "    th2_df[\"mods\"]  = np.random.choice(['CS','95'], th2_df.shape[0], p=[0.5, 0.5])\n",
    "    th1_df[\"mods\"]  = np.random.choice(['CS','95'], th1_df.shape[0], p=[0.5, 0.5])\n",
    "    th2_df[\"mod_values\"] = th2_df[\"mod_values\"] +\"@\"+th2_df[\"mods\"]\n",
    "    th1_df[\"mod_values\"] = th1_df[\"mods\"]\n",
    "    th_df = pd.concat([th1_df, th2_df])\n",
    "    \n",
    "    t1_df= pd.read_csv(c_amt_csv)\n",
    "    t2_df= pd.read_csv(d_amt_csv)\n",
    "    \n",
    "    t1_lst = t1_df['combo'].tolist()\n",
    "    t2_lst = t2_df['combo'].tolist()\n",
    "    t3_lst = t1_lst and t2_lst\n",
    "    th_df[\"combos\"] =  th_df[\"combos\"].astype(str)\n",
    "    th_df[\"combos1\"] = th_df[\"combos\"].apply(lambda x: x.split(\":\"))\n",
    "    safe_lst = []\n",
    "    a = \"\"\n",
    "    th_df['combos1'] = th_df['combos'].str.replace(';', '@')\n",
    "    th_df[\"combos1\"] = th_df[\"combos1\"].apply(lambda x: x.split(\":\"))\n",
    "    th_df['test1'] = th_df['combos1'].apply(lambda x: Counter(x))\n",
    "    test2 = Counter(t3_lst)\n",
    "    th_df['safe'] = th_df['test1'].apply(lambda x: True if sum(x.values()) == sum((x & test2).values()) else False)    \n",
    "    \n",
    "\n",
    "    th_df = th_df.loc[th_df['safe'] == True]\n",
    "    th_df =th_df.drop(columns=[\"combos1\",\"safe\"])\n",
    "\n",
    "    return pd.concat([th_df, final_df])\n",
    "######################################################################################################################\n",
    "def ren_is_bill(final_df):\n",
    "    nr1_df = pd.DataFrame()\n",
    "    ren_df = final_df.loc[(final_df['bad3'] == 'N')]\n",
    "    nr_df = final_df.loc[(final_df['bad3'] == 'Y')]\n",
    "    nr1_df = final_df.loc[(final_df['super_bad'] == 'Y')]\n",
    "    nr_df = pd.concat([nr1_df, nr_df])\n",
    "    nr_df[\"BillingProviderID\"]= nr_df[\"RenderingProviderID\"]\n",
    "    nr_df[\"BillingProviderNPI\"]= nr_df[\"RenderingProviderNPI\"]\n",
    "    nr_df[\"BillingProviderTIN\"]= nr_df[\"RenderingProviderTIN\"]\n",
    "    nr_df[\"BillingProviderSpecialtyCode\"]= nr_df[\"RenderingProviderSpecialtyCode\"]\n",
    "    nr_df[\"BillingProviderTaxonomy\"]= nr_df[\"RenderingProviderTaxonomy\"]\n",
    "    nr_df[\"BillingProviderOrg\"]= nr_df[\"RenderingProviderOrg\"]\n",
    "    nr_df[\"BillingProviderFName\"]= nr_df[\"RenderingProviderFName\"]\n",
    "    nr_df[\"BillingProviderMName\"]= nr_df[\"RenderingProviderMName\"]\n",
    "    nr_df[\"BillingProviderLName\"]= nr_df[\"RenderingProviderLName\"]\n",
    "    nr_df[\"BillingProviderEntityType\"]= nr_df[\"RenderingProviderEntityType\"]\n",
    "    nr_df[\"BillingProviderAddressLine1\"]= nr_df[\"RenderingProviderAddressLine1\"]\n",
    "    nr_df[\"BillingProviderAddressLine2\"]= nr_df[\"RenderingProviderAddressLine2\"]\n",
    "    nr_df[\"BillingProviderCity\"]= nr_df[\"RenderingProviderCity\"]\n",
    "    nr_df[\"BillingProviderState\"]= nr_df[\"RenderingProviderState\"]\n",
    "    nr_df[\"BillingProviderZipCode\"]= nr_df[\"RenderingProviderZipCode\"]\n",
    "    nr_df[\"BillingProviderPhoneNumber\"] = nr_df[\"RenderingProviderPhoneNumber\"]\n",
    "    return pd.concat([ren_df, nr_df])\n",
    "######################################################################################################################\n",
    "def srvc_cnt(final_df, srvs_csv):\n",
    "    final_df = final_df.sort_values(by = \"AdmitDate\")\n",
    "    final_df = final_df.sort_values(by = \"RenderingProviderTIN\")\n",
    "    final_df = final_df.sort_values(by = \"combos\")\n",
    "    final_df['combos'] = final_df['combos'].astype(str)\n",
    "    final_df = final_df.loc[(final_df['combos'] != \"\")]\n",
    "    final_df = final_df.drop_duplicates(subset = ['ClaimID', 'LineNumber'],keep = 'last').reset_index(drop = True)\n",
    "    final_df[\"PreAuthIndicator\"] = final_df[\"hcpcs_code\"].apply(lambda x: preauth(x))\n",
    "    srvs_df = pd.read_csv(srvs_csv)\n",
    "    dfNew = pd.DataFrame()\n",
    "    objMissingCombos = []\n",
    "    dfComboSize = final_df.groupby(['combos'], as_index=False).size()\n",
    "    srvs_df = srvs_df.merge(dfComboSize)\n",
    "    \n",
    "    print(dfComboSize.shape[0])\n",
    "    k = 0\n",
    "    for index, row in dfComboSize.iterrows():\n",
    "        k+=1\n",
    "        if k%600 == 0:\n",
    "            print(k)\n",
    "        combo = row['combos']\n",
    "        dfCombos = srvs_df[srvs_df['combos'] == combo]\n",
    "        objCombos = dfCombos.drop(['percent'], axis = 1).values\n",
    "        objWeights = dfCombos['percent'].tolist()\n",
    "        intSize = row['size']\n",
    "        objCols = dfCombos.drop(['percent'], axis = 1).columns\n",
    "        if len(objCombos) > 0:\n",
    "            dfNewTmp = pd.DataFrame(random.choices(objCombos, objWeights, k = row['size']), columns=objCols)\n",
    "            dfNew = dfNew.append(dfNewTmp)\n",
    "        else:\n",
    "            objMissingCombos.append([combo, 1, intSize])\n",
    "    if len(objMissingCombos) > 0:\n",
    "        dfMissing = pd.DataFrame(objMissingCombos, columns=['combos', 'srvc_cnt', 'size_out'])\n",
    "        dfMissing = dfMissing.loc[dfMissing.index.repeat(dfMissing.size_out)]\n",
    "        dfNew = dfNew.append(dfMissing)\n",
    "    dfSrvcCnts = dfNew[['combos', 'srvc_cnt']].sort_values('combos')\n",
    "    final_df = final_df.sort_values(['combos'])\n",
    "    objSrvcCnts = dfSrvcCnts['srvc_cnt'].tolist()\n",
    "    final_df['srvc_cnt_new'] = objSrvcCnts\n",
    "    return final_df\n",
    "######################################################################################################################\n",
    "def payment(final_df, c_amt_csv, d_amt_csv):\n",
    "    final_df[\"combo\"] = final_df[\"hcpcs_code\"] +\"@\"+final_df[\"mod_values\"]\n",
    "    def splitting2(col_name, num):\n",
    "        col_name = col_name.split('@')\n",
    "        null_vals = num - len(col_name)\n",
    "        null_lst = [\"\"] * null_vals\n",
    "        col_name = col_name + null_lst\n",
    "        return col_name\n",
    "    \n",
    "    #Starting to calculate the payment for each claim on both a line and claim level.\n",
    "    c = pd.read_csv(c_amt_csv)\n",
    "    d = pd.read_csv(d_amt_csv)\n",
    "    c = c.fillna(0)\n",
    "    d = d.fillna(0)\n",
    "    final_df['combo'] = final_df['combo'].astype(str)\n",
    "    final_df['combo'] = final_df[\"combo\"].apply(lambda x: x.split(\"@\"))\n",
    "    final_df['combo'] = final_df['combo'].apply(sorted)\n",
    "    final_df['combo'] = final_df['combo'].transform(lambda x: '@'.join(x))\n",
    "    \n",
    "    scb = 25      #may want to change scale so that it is more in line with reality\n",
    "    sca = .05\n",
    "    cost_dict = c.set_index('combo').to_dict()\n",
    "    \n",
    "    final_df['combo'] = final_df['combo'].fillna(0)\n",
    "    final_df['combo'] = final_df['combo'].astype(str)\n",
    "    final_df = final_df.loc[(final_df['combo'] != \"nan\")]\n",
    "    \n",
    "    t1_df= pd.read_csv(c_amt_csv)\n",
    "    t2_df= pd.read_csv(d_amt_csv)\n",
    "    \n",
    "    t1_lst = t1_df['combo'].tolist()\n",
    "    t2_lst = t2_df['combo'].tolist()\n",
    "    t3_lst = t1_lst and t2_lst\n",
    "    final_df[\"combo\"] = final_df[\"combo\"].astype(str)\n",
    "    comb_lst = final_df[\"combo\"].tolist()\n",
    "    safe = []\n",
    "    for a in comb_lst:\n",
    "        if a in t3_lst:\n",
    "            safe.append(\"T\")\n",
    "        else:\n",
    "            safe.append(\"F\")\n",
    "    final_df[\"safe\"] = safe\n",
    "    final_df =  final_df.loc[(final_df['safe'] == \"T\")]\n",
    "    \n",
    "    final_df['cost'] = final_df['combo'].apply(lambda x: cost_dict[\"avg_charge\"][x])\n",
    "    final_df['real_charge'] = np.random.normal(final_df['cost'], scale = scb, size = final_df.shape[0]).astype(int)\n",
    "    final_df['real_charge'] = final_df['real_charge'].abs()\n",
    "    final_df = final_df.drop(columns=['cost', \"bad1\", \"bad2\"])\n",
    "    paid_dict = d.set_index('combo').to_dict()\n",
    "    final_df['paid'] = final_df['combo'].apply(lambda x: paid_dict[\"avg_paid_pct\"][x])\n",
    "    \n",
    "    final_df['real paid'] = np.random.normal(final_df['paid'], scale = sca, \n",
    "                                             size = final_df.shape[0])*final_df[\"real_charge\"].astype(int)\n",
    "    final_df['real paid'] = final_df['real paid'].abs()\n",
    "    \n",
    "    final_df = final_df.drop(columns=['paid'])\n",
    "    final_df[\"real paid\"] = final_df[\"real paid\"].astype('int64', copy=False, errors='ignore')\n",
    "    final_df[\"claim\"] = final_df[\"ClaimID\"]+\"@\"+final_df[\"ren_id\"]\n",
    "    \n",
    "    final_df_1 = final_df\n",
    "    final_df_1.groupby(['claim']).agg({'real_charge': sum, 'real paid': sum}).reset_index()\n",
    "    \n",
    "    final_df[\"total charged\"] = final_df_1[\"real_charge\"]\n",
    "    final_df[\"total paid\"] = final_df_1[\"real paid\"]*1.4\n",
    "    final_df = final_df.drop(columns=['claim'])\n",
    "    final_df['diag_values'] = final_df['diag_values'].apply(lambda x: x[0:25])\n",
    "    final_df['diag_values'] = final_df['diag_values'].apply(lambda x: '@'.join(x))\n",
    "    final_df['mod_values'] = final_df['mod_values'].apply(lambda x: splitting2(x, 6))\n",
    "    \n",
    "    final_df['diag_values'] = final_df['diag_values'].apply(lambda x: splitting2(x, 25))\n",
    "    final_df['diag_values'] = final_df['diag_values'].apply(lambda x: x[0:25])\n",
    "    \n",
    "    final_df[diag_col_lst] = pd.DataFrame(final_df.pop('diag_values').values.tolist(), index=final_df.index)\n",
    "    \n",
    "    final_df[[\"MOD1\",\"MOD2\",\"MOD3\",\"MOD4\",\"MOD5\",\"MOD6\"]] = pd.DataFrame(final_df.pop('mod_values').values.tolist(),\n",
    "                                                                     index=final_df.index)\n",
    "\n",
    "    \n",
    "    final_df = final_df.rename(columns = {'pat_gender':'PatientGender', \"place_of_service\": \"LinePOS\",\n",
    "                                          \"total paid\":\"LineTotPaid\", \"total charged\":\"LineCharges\", \n",
    "                                          \"hcpcs_code\":\"LineProcCode\"})\n",
    "    \n",
    "    final_df['MOD1']=final_df['MOD1'].replace(['None'], '')\n",
    " \n",
    "    final_df[\"LineTotPaid\"] = final_df[\"LineTotPaid\"].astype('int64', copy=False, errors='ignore')   \n",
    "    final_df[\"LineCOBPay\"] = final_df[\"LineCOBPay\"]*final_df[\"LineTotPaid\"]\n",
    "    \n",
    "    par_df = final_df.loc[(final_df['ContractedFlag'] == 'Subscriber')]\n",
    "    np_df = final_df.loc[(final_df['ContractedFlag'] != 'Subscriber')]\n",
    "    np_df[\"LineAmountCoinsurance\"] = np.random.randint(20,50, size=final_df.shape[0])/100\n",
    "    par_df[\"LineAmountCoinsurance\"] = 0\n",
    "    final_df = pd.concat([par_df, np_df])\n",
    "    \n",
    "    final_df[\"LineAmountCoinsurance\"] = np.random.randint(0,10, size=final_df.shape[0])/100\n",
    "    final_df[\"LineAmountDeductible\"]= np.random.randint(0,20, size=final_df.shape[0])/100\n",
    "    final_df[\"LineAmountCopay\"]=[random.randrange(5,26,5)for i in range(final_df.shape[0])]\n",
    "    final_df[\"LinePayerPaid\"]=final_df[\"LineTotPaid\"]-final_df[\"LineAmountCopay\"]\n",
    "    final_df[\"LineAmountCoinsurance\"] = final_df[\"LineAmountCoinsurance\"] * final_df[\"LinePayerPaid\"]//1\n",
    "    final_df[\"LinePayerPaid\"]=final_df[\"LineTotPaid\"]-final_df[\"LineAmountCopay\"]- final_df[\"LineAmountCoinsurance\"]\n",
    "    final_df['LinePayerPaid'] = final_df['LinePayerPaid'].abs()\n",
    "    final_df[\"LineAmountDeductible\"] = final_df[\"LineAmountDeductible\"] * final_df[\"LinePayerPaid\"]//1\n",
    "    #function definitions for when the claim might not get paid, ie plastic surgery or otherwise rejected.\n",
    "\n",
    "    final_df[\"LinePayerPaid\"] = final_df.apply(nopay, axis=1)\n",
    "    final_df[\"LineServiceStatusCategoryCode\"] = final_df.apply(nopay2, axis=1)\n",
    "    final_df[\"PayeeType\"] = final_df.apply(nopay3, axis=1)\n",
    "    final_df[\"PayeeType\"] = final_df.apply(nopay4, axis=1)\n",
    "    final_df[\"SubscriberState\"]= final_df[\"PatientState\"]\n",
    "    final_df['LineAmountCoinsurance'] = final_df['LineAmountCoinsurance'].abs()\n",
    "    final_df['LineAmountDeductible'] = final_df['LineAmountDeductible'].abs()\n",
    "    final_df[\"LinePatientPaid\"] = final_df[\"LineAmountCopay\"] + final_df[\"LineAmountDeductible\"] + final_df[\"LineAmountCoinsurance\"]\n",
    "    final_df[\"LineTotPaid\"] = final_df[\"LinePatientPaid\"]+final_df[\"LineCOBPay\"]+ final_df[\"LinePayerPaid\"]\n",
    "    final_df[\"HdrCOBPay\"] = \"0\"\n",
    "    final_df = final_df.sort_index(ascending=True)\n",
    "    \n",
    "    hdr_df= final_df[[\"ClaimID\",\"LineTotPaid\", \"LineAmountCopay\", \"LinePayerPaid\",  \"LineCharges\",\"LinePatientPaid\" ]]\n",
    "    \n",
    "    hdr_df = hdr_df.groupby(['ClaimID'], as_index=False).agg({'LineTotPaid': 'sum','LinePatientPaid': 'sum',\n",
    "                                                              'LineAmountCopay': 'sum','LinePayerPaid': 'sum',\n",
    "                                                              'LineCharges': 'sum'})\n",
    "    hdr_df.rename(columns={'LineTotPaid':'HdrPay','LinePayerPaid': 'HdrPayerPay',\n",
    "                           'LineAmountCopay': 'HdrPatientCopay', 'LineCharges': 'HdrCharges',\n",
    "                           'LinePatientPaid': 'HdrPatientPay'}, inplace=True)\n",
    "    \n",
    "    final_df = pd.merge(final_df, hdr_df, on='ClaimID')\n",
    "\n",
    "    final_df['LineTotPaid'] = final_df['LineTotPaid'].astype(float)\n",
    "    return final_df.loc[(final_df['LineTotPaid'] <= 1000000.0)]\n",
    "######################################################################################################################\n",
    "def pat_relationship(final_df):\n",
    "    df1 = final_df.loc[(final_df['PatientRelationship'] == 'Subscriber')]\n",
    "    df2 = final_df.loc[(final_df['PatientRelationship'] != 'Subscriber')]\n",
    "    df1['SubscriberLName'] = df1['PatientLName']\n",
    "    df1['SubscriberFName'] = df1['PatientFName']\n",
    "    df1['SubscriberMName'] = df1['PatientMName']\n",
    "    df1['PatientID'] = df1[\"SubscriberID\"]\n",
    "    final_df = pd.concat([df1, df2])\n",
    "    df1 = final_df.loc[(final_df['RenderingProviderState'] == '')]\n",
    "    df2 = final_df.loc[(final_df['RenderingProviderState'] != '')]\n",
    "    df1['RenderingProviderState'] = df1[\"PatientState\"]\n",
    "    return pd.concat([df1, df2])\n",
    "######################################################################################################################\n",
    "def save_files(final_df, folder, taxonomy):\n",
    "    rend_file=pd.DataFrame()\n",
    "    rend_file[\"etl_provider_key\"]=\"\"\n",
    "    rend_file[\"ProviderNPI\"]=final_df[\"RenderingProviderNPI\"] \n",
    "    rend_file[\"predicted_specialty_id_1\"] = taxonomy\n",
    "    rend_file[\"number_of_predicted_specialties\"]=\"1\"\n",
    "    rend_file[\"specialty_1_probability_value\"]=\"\"\n",
    "    rend_file[\"predicted_specialty_id_2\"]=\"\"\n",
    "    rend_file[\"specialty_2_probability_value\"]=\"\"\n",
    "    rend_file[\"predicted_specialty_id_3\"]=\"\"\n",
    "    rend_file[\"specialty_3_probability_value\"]=\"\"\n",
    "    rend_file[\"customer_provided_specialty_desc\"]=\"\"\n",
    "    rend_file[\"npi_reg_primary_taxonomy_code\"]=\"\"\n",
    "    rend_file[\"used_cust_specialty_flg\"]=\"\"\n",
    "    rend_file = rend_file.drop_duplicates(subset = ['ProviderNPI'],keep = 'last').reset_index(drop = True)\n",
    "\n",
    "    bill_prov_file=pd.DataFrame()\n",
    "    bill_prov_file['etl_provider_key'] = ''\n",
    "    bill_prov_file['ProviderNPI'] = final_df['BillingProviderNPI']\n",
    "    bill_prov_file['predicted_specialty_id_1'] = final_df['BillingProviderSpecialtyCode']\n",
    "    bill_prov_file['number_of_predicted_specialties'] = '1'\n",
    "    bill_prov_file['specialty_1_probability_value'] = ''\n",
    "    bill_prov_file['predicted_specialty_id_2'] = ''\n",
    "    bill_prov_file['specialty_2_probability_value'] = ''\n",
    "    bill_prov_file['predicted_specialty_id_3'] = ''\n",
    "    bill_prov_file['specialty_3_probability_value'] = ''\n",
    "    bill_prov_file['customer_provided_specialty_desc'] = ''\n",
    "    bill_prov_file['npi_reg_primary_taxonomy_code'] = ''\n",
    "    bill_prov_file['used_cust_specialty_flg'] = ''\n",
    "    bill_prov_file[\"cust_claim_specialty_desc\"]=\"\"\n",
    "    bill_prov_file[\"cdx_claim_id\"]=\"\"\n",
    "    bill_prov_file[\"claim_specialty_id\"]=\"\"\n",
    "    bill_prov_file[\"claim_specialty_probability_value\"]=\"\"\n",
    "    bill_prov_file = bill_prov_file.drop_duplicates(subset = ['ProviderNPI'],keep = 'last').reset_index(drop = True)\n",
    "\n",
    "    rend_file.to_csv(\"Out/Professional/\"+folder+\"_provider_file1.csv\", index=False)\n",
    "    bill_prov_file.to_csv(\"Out/Professional/\"+folder+\"_provider_file2.csv\", index=False)\n",
    "    final_df = final_df.sample(frac=1)\n",
    "    final_df = final_df.sort_values(by=['ClaimID'])\n",
    "    final_df = final_df.sort_values(by=['ClaimPaidDate'])\n",
    "\n",
    "    final_df[\"Date\"] = final_df[\"ClaimPaidDate\"]\n",
    "    final_df[\"Date\"] = final_df[\"Date\"].astype(str)\n",
    "    final_df[\"Date\"] = final_df[\"Date\"].str[:7]\n",
    " \n",
    "    final_df = final_df.drop_duplicates(subset = ['ClaimID', 'LineNumber'],keep = 'last').reset_index(drop = True)\n",
    "    #save data to sagemaker based on month of claim\n",
    "    \n",
    "    out = final_df\n",
    "    out = out.drop(columns=['Date'])\n",
    "    i = 0\n",
    "    j = 0\n",
    "    k = 0\n",
    "    c = \"\"\n",
    "    for index, row in final_df.iterrows():\n",
    "        j+=1\n",
    "        if not (c ==row[\"Date\"]):\n",
    "            c = row[\"Date\"]\n",
    "            if k!=0:\n",
    "                strBucket = 'data-science-synthdata-dev'\n",
    "                strTestFile = \"Out/Professional/\"+str(i)+\"/\"+folder+\"_Month_\"+str(i)+'.csv'\n",
    "                strOutFile = 's3://{}/{}'.format(strBucket, strTestFile)\n",
    "                out.iloc[k:j-1].to_csv(strTestFile, index=False)\n",
    "            k = j\n",
    "            i+=1\n",
    "    final_df.to_csv(\"Out/Professional/\"+str(folder)+\".csv\",  index=False)#sep=\"|\",\n",
    "    #i = 1\n",
    "    #j = 0\n",
    "    #k = 0\n",
    "    #c = \"\"\n",
    "    #for index, row in dfDate.iterrows():\n",
    "    #    j+=1\n",
    "    #    if not (c ==row[\"date_out\"]):\n",
    "    #        c = row[\"date_out\"]\n",
    "    #        print('c: ' + str(c))\n",
    "    #        strBucket = 'data-science-synthdata-dev'\n",
    "    #        strTestFile = \"Out/Professional/Month_\" + str(i) + '.csv'\n",
    "    #        strOutFile = 's3://{}/{}'.format(strBucket, strTestFile)\n",
    "    #        print(strOutFile)\n",
    "    #        # out.iloc[k:j-1].to_csv(strTestFile, index=False)\n",
    "    #        k = j\n",
    "    #        i+=1\n",
    "######################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a07f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ff7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syn_data(taxonomy,  folder, div):\n",
    "    num_claims_csv =\"Input/Professional/\"+ folder+\"/num_claim_df.csv\"\n",
    "    all_combo_csv = \"Input/Professional/\"+ folder+\"/all_combinations_age.csv\"\n",
    "    location_csv = \"Input/Professional/\"+ folder+\"/location_data.csv\"\n",
    "    c_amt_csv = \"Input/Professional/\"+ folder+\"/c.csv\"\n",
    "    d_amt_csv =\"Input/Professional/\"+  folder+\"/d.csv\"\n",
    "    srvs_csv = \"Input/Professional/\"+  folder+\"/srvc_cnts.csv\"\n",
    "    def splitting(name, delineator = \"@\"):\n",
    "        final_df[name] = final_df[name].apply(lambda x: x.split(delineator))\n",
    "\n",
    "    def ren_splitting(name, delineator = \"@\"):\n",
    "        rendering_df[name] = rendering_df[name].apply(lambda x: x.split(delineator))\n",
    "\n",
    "    rendering_df = pd.DataFrame()\n",
    "    num_claims_df = pd.read_csv(num_claims_csv)\n",
    "    num_providers = num_claims_df.shape[0]//(div)#may want to edit this, going lower than about 10 requires more RAM\n",
    "    \n",
    "    ren_df = pd.read_csv(\"Input/rendering_prov.csv\")\n",
    "    a = \"prof_\"+folder.lower()\n",
    "    ren_df= ren_df[ren_df[\"Tax\"].str.contains(a)]\n",
    "    ren_df = ren_df.drop(columns=[\"Tax\"])\n",
    "    ren_df['prob'] = (ren_df['prob'] / ren_df['prob'].sum()) \n",
    "    rend_dict = dict(ren_df.values)\n",
    "\n",
    "    #ren_lst = ren_df[\"REN\"].tolist()\n",
    "    ren_df = pd.read_csv(\"Input/billing_prov.csv\")\n",
    "    ren_df= ren_df[ren_df[\"Tax\"].str.contains(a)]\n",
    "    ren_df = ren_df.drop(columns=[\"Tax\", \"Unnamed: 0\"])\n",
    "    ren_df['prob'] = (ren_df['prob'] / ren_df['prob'].sum()) \n",
    "    billing_prov_dict = dict(ren_df.values)\n",
    "\n",
    "\n",
    "    rendering_df = ren_tax(rendering_df, taxonomy, num_providers)\n",
    "    rendering_df['ren_id'] = range(1, 1+len(rendering_df))\n",
    "    \n",
    "    rendering_df[\"ren\"] =  np.random.choice(list(rend_dict.keys()),rendering_df.shape[0], p=list(rend_dict.values()))\n",
    "    #np.random.choice(ren_lst, rendering_df.shape[0])find me\n",
    "    ren_splitting(\"ren\")\n",
    "    rendering_df[ren_col_lst] = pd.DataFrame(rendering_df.pop('ren').values.tolist(), index=rendering_df.index)\n",
    "    \n",
    "    #Probability of number of claims per provider\n",
    "    #We'll eventually bring inference (of number of claims for each provider) from all customers for each specialty.\n",
    "    #Based on MonteCarlo simulation from the empirical data\n",
    "    \n",
    "    ## User defined for each specialty\n",
    "    num_iter = 100000\n",
    "    num_claims_avg_lst = []\n",
    "    \n",
    "    arr_claims = num_claims_df.num_claims.values\n",
    "    n_C = round(0.1*len(num_claims_df))\n",
    "    for ii in range(0, num_iter):\n",
    "        num_claims_avg_lst.append(round(arr_claims[np.random.choice(arr_claims.shape[0], n_C)].mean()))\n",
    "    \n",
    "    dist_mean = np.round(np.mean(num_claims_avg_lst))\n",
    "    dist_std = np.round(np.std(num_claims_avg_lst))\n",
    "    group = [int(round(x)) for x in np.random.normal(loc=dist_mean, scale=3*dist_std,size=num_providers)]\n",
    "    group = [int(x * 1.3) for x in group]\n",
    "    group[0] = int(group[0]*1.3)\n",
    "    group[1] = int(group[1]*1.3)\n",
    "    group[2] = int(group[2]*1.6)\n",
    "    group[3] = int(group[3]*1.6)\n",
    "    group = [abs(x) for x in group]\n",
    "    \n",
    "    ### Determine probability of different combinations \n",
    "    ### We'll eventually bring inference (of number of claims for each combination) from all customers for\n",
    "    #each specialty.\n",
    "    all_combinations = pd.read_csv(all_combo_csv)\n",
    "    all_combinations[\"prob_claims\"] =all_combinations[\"prob_claims\"].astype(float)\n",
    "    all_combinations[\"prob_claims\"] = all_combinations[\"prob_claims\"] * all_combinations[\"prob_claims\"]\n",
    "    all_combinations= all_combinations[all_combinations[\"combos\"].str.contains(\"INT\")==False]\n",
    "    all_combinations= all_combinations[all_combinations[\"combos\"].str.contains(\"1P\")==False]\n",
    "    all_combinations= all_combinations[all_combinations[\"combos\"].str.contains(\"09036\")==False]\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    dfDiagCombos = all_combinations.copy()\n",
    "    dfGrouped = dfDiagCombos.groupby(['combos', 'place_of_service', 'pat_gender', 'age_bin'],\n",
    "                                     as_index=False).agg({'num_claims': 'sum'})\n",
    "    dfDiagCombos = dfDiagCombos.merge(dfGrouped.rename(columns={'num_claims': 'tot_claims'}))\n",
    "    dfDiagCombos['prob_diag'] = dfDiagCombos['num_claims']/dfDiagCombos['tot_claims']\n",
    "\n",
    "    all_combinations = all_combinations.groupby(['combos', 'place_of_service', 'pat_gender', 'age_bin'],\n",
    "                                                as_index=False).agg({'num_claims': 'sum'})\n",
    "    all_combinations['prob_claims'] = all_combinations['num_claims']/all_combinations['num_claims'].sum()\n",
    "    \n",
    "    t1_df= pd.read_csv(c_amt_csv)\n",
    "    t2_df= pd.read_csv(d_amt_csv)\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    t1_df['combo1'] = t1_df['combo'].apply(lambda x: x.split('@'))\n",
    "    t1_df['combo2'] = t1_df.apply(lambda x: sorted(x['combo1'], key=len, reverse=True), axis=1)\n",
    "    t1_df['combo'] = t1_df['combo2'].apply(lambda x: '@'.join(x))\n",
    "    t2_df['combo1'] = t2_df['combo'].apply(lambda x: x.split('@'))\n",
    "    t2_df['combo2'] = t2_df.apply(lambda x: sorted(x['combo1'], key=len, reverse=True), axis=1)\n",
    "    t2_df['combo'] = t2_df['combo2'].apply(lambda x: '@'.join(x))\n",
    "    \n",
    "    t1_lst = t1_df['combo'].tolist()\n",
    "    t2_lst = t2_df['combo'].tolist()\n",
    "    t3_lst = t1_lst and t2_lst\n",
    "    del t1_df\n",
    "    del t2_df\n",
    "    all_combinations[\"combos1\"] = all_combinations[\"combos\"].apply(lambda x: x.split(\":\"))\n",
    "    safe_lst = []\n",
    "    a = \"\"\n",
    "    all_combinations['combos1'] = all_combinations['combos'].str.replace(';', '@')\n",
    "    all_combinations[\"combos1\"] = all_combinations[\"combos1\"].apply(lambda x: x.split(\":\"))\n",
    "    all_combinations['test1'] = all_combinations['combos1'].apply(lambda x: Counter(x))\n",
    "    test2 = Counter(t3_lst)\n",
    "    all_combinations['safe'] = all_combinations['test1'].apply(lambda x: True if sum(x.values()) == \n",
    "                                                               sum((x & test2).values()) else False)    \n",
    "\n",
    "    sel_combinations = all_combinations.loc[all_combinations['safe'] == True]\n",
    "    sel_combinations =sel_combinations.drop(columns=[\"combos1\",\"safe\"])\n",
    "    ## Unique combinations and corresponding weights\n",
    "    comb_list = sel_combinations.drop(['num_claims', 'prob_claims'], axis = 1).values\n",
    "    weights = sel_combinations['prob_claims'].tolist()\n",
    "    sel_columns= sel_combinations.drop(['num_claims', 'prob_claims'], axis = 1).columns\n",
    "    print(num_providers)\n",
    "    cdf = [weights[0]]\n",
    "    for w in weights[1:]:\n",
    "        cdf.append(cdf[-1]+w)\n",
    "\n",
    "    final_df_list = []\n",
    "    random_outliers = random.sample(range(0, num_providers), int(round(0.001*num_providers)))\n",
    "    random_outliers.append(0)\n",
    "    random_outliers.append(1)\n",
    "    random_outliers.append(2)\n",
    "    for ii in range(0, num_providers):\n",
    "        if ii in random_outliers:\n",
    "            tmp = pd.DataFrame(random.choices(comb_list, k=group[ii]), columns = sel_columns)\n",
    "        else:\n",
    "            tmp = pd.DataFrame(random.choices(comb_list, cum_weights = cdf, k=group[ii]), columns = sel_columns)\n",
    "        tmp['ren_id'] = str(ii+1)\n",
    "        final_df_list.append(tmp)\n",
    "\n",
    "    final_df = pd.concat(final_df_list)\n",
    "\n",
    "    print('getting diag combos')\n",
    "\n",
    "    final_df = getting_final_combos(final_df, dfDiagCombos)\n",
    "    print('done getting diag combos')\n",
    "    \n",
    "    final_df = def_bad_provs(final_df)\n",
    "\n",
    "    ##################### AI Alerts\n",
    "    final_df= know_your_mohs(final_df, all_combo_csv, folder, c_amt_csv, d_amt_csv)\n",
    "\n",
    "    ###covid testing drive thru\n",
    "    final_df = covid_drive_thru(final_df)\n",
    "\n",
    "    final_df[\"combos\"] = final_df[\"combos\"].apply(lambda x: x.split(\":\"))\n",
    "    \n",
    "    final_df['ClaimID'] = range(1, 1 + len(final_df))\n",
    "    final_df['ClaimID'] = final_df['ClaimID'].astype(str)\n",
    "    final_df[\"1\"] = str(taxonomy)\n",
    "    final_df[\"1\"]=final_df[\"1\"].apply(lambda x: '{0:0>3}'.format(x))\n",
    "    final_df['ClaimID'] = \"1\" + final_df['1'] + final_df['ClaimID']\n",
    "    final_df = final_df.drop(columns=['1'])    \n",
    "    zip_combinations = state_info(rendering_df, location_csv)\n",
    "    zip_df = zip_combinations.copy()\n",
    "    \n",
    "    \n",
    "    comb_list = zip_combinations.drop(['prob_combo'], axis = 1).values\n",
    "    weights = zip_combinations['prob_combo'].tolist()\n",
    "    zip_length = rendering_df.shape[0] + 10\n",
    "    zip_df = pd.DataFrame(random.choices(comb_list, weights, k = zip_length), columns = zip_df.drop(['prob_combo'],\n",
    "                                                                                                  axis = 1).columns)\n",
    "    rend_zips = zip_df['renderingproviderzipcode'].tolist()\n",
    "    rendering_df['renderingproviderzipcode'] = np.random.choice(rend_zips, rendering_df.shape[0])\n",
    "    \n",
    "    rend_zip_df = rendering_df.merge(zip_df.drop_duplicates(), on = 'renderingproviderzipcode')\n",
    "    dfRendChoices = pd.DataFrame()\n",
    "    rend_tin = rend_zip_df['RenderingProviderTIN'].unique()\n",
    "    i = 0\n",
    "    for tin in rend_tin:\n",
    "        comb_list = rend_zip_df[rend_zip_df['RenderingProviderTIN'] == tin].values\n",
    "    \n",
    "        dfTmp = pd.DataFrame(random.choices(comb_list, k=random.choices([1, 2, 3,4,5], k = 1)[0]),\n",
    "                             columns = rend_zip_df.columns)\n",
    "        i += 1\n",
    "        dfTmp[\"ren_id\"] = str(i)\n",
    "        dfRendChoices = dfRendChoices.append(dfTmp)\n",
    "    \n",
    "    del rend_zip_df\n",
    "    del zip_df\n",
    "    print(\"Zipping\")\n",
    "    dfRendChoices = find_zips(dfRendChoices)\n",
    "    \n",
    "    rend = dfRendChoices.values.tolist()\n",
    "    out = []\n",
    "    c = []\n",
    "    f = \"\"\n",
    "    for a in rend:\n",
    "        if f == a[14]:\n",
    "            c.append(a)\n",
    "        else:\n",
    "            out.append(c)\n",
    "            c = [a]\n",
    "            f = (a[14]) \n",
    "    \n",
    "    out.append(c)\n",
    "    out.pop(0)\n",
    "    print(len(out))\n",
    "\n",
    "    final_df = final_df.loc[(final_df['ren_id'] <= len(out))]\n",
    "    final_df[\"Ren\"] = final_df[\"ren_id\"].apply(lambda x: random.choice(out[int(x)-1]))\n",
    "\n",
    "    rend = (list(dfRendChoices.columns))\n",
    "    del dfRendChoices\n",
    "    \n",
    "    num_lines= final_df.shape[0]\n",
    "    #This creates a dataframe of useful info regarding the synthedic billing providers creating names \n",
    "    #and info randomly based on value specifications.\n",
    "    ## find me\n",
    "    \n",
    "\n",
    "    billing_list = bill_info(num_providers, taxonomy, billing_prov_dict)\n",
    "    \n",
    "    # basic df for info on groups for the patinets to be part of. Probably will move to patient data\n",
    "    group_lst = group_info(num_providers)\n",
    "\n",
    "    \n",
    "    print(\"end of billing prov info generation\")\n",
    "\n",
    "    #Null data for Professional which is populated in Dental or facility data or has been decided to not \n",
    "    #be important in the data.\n",
    "    final_df[\"BillingProviderSpecialtyCode\"]=str(taxonomy)\n",
    "    final_df[\"RenderingProviderSpecialtyCode\"]=str(taxonomy)\n",
    "    \n",
    "    final_df[\"InterPlan\"] = np.random.choice([\"Host\", \"ITS\", \"nonITS\"], num_lines, p = [0.4, 0.3,0.3])\n",
    "    final_df[\"ASOFlag\"] = np.random.choice([\"Y\", \"N\"], num_lines, p = [0.5, 0.5])\n",
    "    final_df[\"CapEpcIndicator\"] = np.random.choice([\"Y\", \"N\"], num_lines, p = [0.2, 0.8])\n",
    "    final_df[\"DenyFlag\"] = np.random.choice([\"Y\", \"N\"], num_lines, p = [0.003, 0.997])\n",
    "    final_df[\"NumofAdjustments\"]= np.random.exponential(.5, size=num_lines).astype(int)#can be edited to change rate\n",
    "    #of edits violations\n",
    "    final_df[\"ReferringProviderNPI\"] = np.random.randint(1000000000, 9999999999, size = final_df.shape[0])\n",
    "    final_df[\"InterPlan\"] = np.random.choice([\"ITS\", \"nonITS\", \"HOST\"], num_lines, p = [0.3, 0.4, 0.3])\n",
    "    final_df[\"ReferringProviderNPI\"] = np.random.randint(1000000000, 9999999999, size = final_df.shape[0])\n",
    "\n",
    "    final_df[\"ClaimType\"] = \"P\" #P for profesional, D for dental, F for facility\n",
    "    final_df[\"PayeeType\"] = \"Prov\" \n",
    "    final_df[\"ICDVersion\"] = \"ICD-10\" \n",
    "    \n",
    "    final_df[null_lst] =''#This creates all the columns that are null in the df\n",
    "\n",
    "    #Chooses billing provider and group for the calim, again likely to change this as group sholuld be attached \n",
    "    #to patient.\n",
    "    final_df[\"Billing\"] = np.random.choice(billing_list, size = num_lines)\n",
    "    final_df[\"Group\"] = np.random.choice(group_lst, size = num_lines)\n",
    "  \n",
    "\n",
    "    #Below calculates dates for the claims and is based on at most 2 years ago and a minimum start date of \n",
    "    #1 month prior to current to mainatin 24 months of coverage.\n",
    "    final_df = claim_date(final_df)\n",
    "\n",
    "    #Function to split concatenated cells into lists to be better used.\n",
    "    splitting(\"Billing\")\n",
    "    splitting(\"Group\")\n",
    "    print(\"End of Splitting\")\n",
    "    \n",
    "    #Splitting lists made directly above into columns in the data.\n",
    "    final_df[bill_col_lst] = pd.DataFrame(final_df.pop('Billing').values.tolist(), index = final_df.index)\n",
    "    final_df[[\"GroupName\",\"GroupNumber\"]] =pd.DataFrame(final_df.pop('Group').values.tolist(), index = final_df.index)\n",
    "    final_df[rend] = pd.DataFrame(final_df.pop('Ren').values.tolist(), index = final_df.index)\n",
    "    \n",
    "    #Set address info to null if no zip code.\n",
    "    final_df = prov_zips(final_df)\n",
    "    \n",
    "    final_out = pd.DataFrame()\n",
    "    print(final_df.shape[0])\n",
    "    # Split final DF based on age and gender so that a patient can be reasonably assigned to match the claim info.\n",
    "    print(\"States for patients\")\n",
    "    final_df = pat_states(final_df)\n",
    "    print(final_df.shape[0])\n",
    "    \n",
    "    splitting(\"Pat\")\n",
    "\n",
    "    final_df[pat_col_lst] = pd.DataFrame(final_df.pop('Pat').values.tolist(), index=final_df.index)\n",
    "\n",
    "    #Set address info to null if no zip code.\n",
    "    final_df = null_bill_zips(final_df)\n",
    "    final_df =  final_df.loc[(final_df['PatientID'] != \"\")]\n",
    "    \n",
    "    bad_prov_lst(final_df, folder)\n",
    "\n",
    "    dfZip = final_df[['RenderingProviderNPI','PatientZipCode']].drop_duplicates('RenderingProviderNPI')\n",
    "    final_df['RenderingProviderZipCode'] = final_df[['RenderingProviderNPI']].merge(dfZip,\n",
    "                                                            on = 'RenderingProviderNPI')['PatientZipCode'].values[:]\n",
    "    final_df['BillingProviderZipCode'] = final_df['RenderingProviderZipCode'].values[:]\n",
    "    print(\"start of peppering\")\n",
    "    \n",
    "    #calculates insurance type based on age, elderly on medicare and Commercial plus whereas younger patients\n",
    "    #have mainly Commercial and Commercial plus with only a few on medicare.\n",
    "    final_df = insurance_type(final_df)\n",
    "\n",
    "    combo_list = final_df[\"combos\"].tolist()\n",
    "    final_df[\"APCCode\"] = \"\"\n",
    "    #Area for peppering new data with detector problems, done prior to line seperation\n",
    "    final_df = claim_peppering(final_df)\n",
    "    print(\"End of Peppering\")\n",
    "    final_df = line_nums(final_df)#this fucntion splits claims into lines and makes a version of combos \n",
    "    #for the next steps\n",
    "    print(\"Claim Line seperated\")\n",
    "    #telehealth\n",
    "    final_df = telehealth_fraud(final_df, c_amt_csv, d_amt_csv)\n",
    "\n",
    "    #Accounts for the case rendering is billing prov\n",
    "    final_df = ren_is_bill(final_df)\n",
    "    \n",
    "    final_df = srvc_cnt(final_df, srvs_csv)\n",
    "\n",
    "    print(\"Line units merged\")\n",
    "    final_df = final_df.rename(columns={'srvc_cnt_new': 'LineUnits', 'combos': 'combo'})\n",
    "\n",
    "    print(\"Start of payment\")\n",
    "    final_df = payment(final_df, c_amt_csv, d_amt_csv)\n",
    "    \n",
    "    print(\"Payments done\")\n",
    "    final_df[\"NumofAdjustments\"]=final_df[\"NumofAdjustments\"].astype(int)\n",
    "    final_df[\"BillingProviderPhoneNumber\"]=final_df[\"BillingProviderPhoneNumber\"].astype(int)\n",
    "    final_df[\"LineNumber\"]=final_df[\"LineNumber\"].astype(int)\n",
    "    final_df[\"RenderingProviderPhoneNumber\"]=final_df[\"RenderingProviderPhoneNumber\"].astype(int)\n",
    "\n",
    "    final_df = pat_relationship(final_df)\n",
    "    \n",
    "    final_df = final_df.drop_duplicates(subset = ['ClaimID', 'LineNumber'])\n",
    "\n",
    "    final_df[\"LineNumber\"]= final_df[\"LineNumber\"].astype(int)\n",
    "    final_df[\"AccidentDate\"]=final_df[\"AccidentDate\"].fillna(\"\")\n",
    "    \n",
    "    final_df=final_df.replace([np.inf, -np.inf], np.nan, inplace=False)\n",
    "    #reorders the columns to fit for data spec.\n",
    "    final_df = final_df[ordered_col_lst]\n",
    "    #2 required files being made for data loading\n",
    "    \n",
    "    print(\"Saving files\")\n",
    "    \n",
    "    save_files(final_df, folder, taxonomy)    \n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20d1f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.call(['bash', 'Test.sh'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af165c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmem(total=16335130624, available=15301939200, percent=6.3, used=725520384, free=12770697216, active=1467392000, inactive=1845346304, buffers=174460928, cached=2664452096, shared=90112, slab=140378112)\n",
      "0\n",
      "870\n",
      "getting diag combos\n",
      "done getting diag combos\n",
      "Zipping\n",
      "819\n",
      "end of billing prov info generation\n",
      "End of Splitting\n",
      "41668\n",
      "States for patients\n",
      "41591\n",
      "start of peppering\n",
      "End of Peppering\n",
      "Claim Line seperated\n",
      "4859\n",
      "600\n",
      "1200\n",
      "1800\n",
      "2400\n",
      "3000\n",
      "3600\n",
      "4200\n",
      "4800\n",
      "Line units merged\n",
      "Start of payment\n",
      "Payments done\n",
      "Saving files\n",
      "1\n",
      "785\n",
      "getting diag combos\n",
      "done getting diag combos\n",
      "Zipping\n",
      "734\n",
      "end of billing prov info generation\n",
      "End of Splitting\n",
      "71204\n",
      "States for patients\n",
      "71106\n",
      "start of peppering\n"
     ]
    }
   ],
   "source": [
    "#order for function\n",
    "#taxonomy \n",
    "#Folder\n",
    "import psutil\n",
    "print(psutil.virtual_memory())\n",
    "print(\"0\")\n",
    "fin = syn_data(6, 'Anesthesiology',50)\n",
    "print(\"1\")\n",
    "syn_data(22, 'Counseling_Psychology',50)\n",
    "print(\"2\")\n",
    "fin = syn_data(31, 'Emergency_Medicine',50)\n",
    "print(\"3\")\n",
    "try:fin = syn_data(66,'Nuclear_Radiology',50)\n",
    "except:print(\"Nuclear_Radiology failed#########################\")\n",
    "try:syn_data(66,'Nuclear_Radiology',50)\n",
    "except: print(\"Nuclear_Radiology failed########################\")\n",
    "print(\"4\")\n",
    "try:syn_data(70, 'Obstetrics_Gynecology',50)\n",
    "except:print(\"Obstetrics_Gynecology failed#########################\")\n",
    "print(\"5\")\n",
    "try:syn_data(72, 'Ophthalmology',50)\n",
    "except:print(\"Ophthalmology failed#########################\")\n",
    "print(\"6\")\n",
    "try:syn_data(75, 'Orthopaedic_Surgery',50)\n",
    "except:print(\"Orthopaedic_Surgery failed#########################\")\n",
    "print(\"7\")\n",
    "try:fin = syn_data(84, 'Pathology',50)\n",
    "except:print(\"Pathology failed#########################\")\n",
    "print(\"8\")\n",
    "try:syn_data(121, 'Psychiatry_Neurology',50)\n",
    "except:print(\"Psychiatry_Neurology failed#########################\")\n",
    "print(\"9\")\n",
    "try: fin = syn_data(108, 'Pediatrics',50)\n",
    "except: print(\"Pediatrics failed#########################\")\n",
    "print(\"10\")\n",
    "\n",
    "i = 0\n",
    "while i < 23:\n",
    "    i+=1\n",
    "    print(i)\n",
    "    all_filenames = [\"Out/Professional/\"+str(i)+\"/Anesthesiology_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Counseling_Psychology_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Emergency_Medicine_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Nuclear_Radiology_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Obstetrics_Gynecology_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Ophthalmology_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Orthopaedic_Surgery_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Pathology_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Pediatrics_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Psychiatry_Neurology_Month_\"+str(i)+\".csv\"]\n",
    "    df = pd.concat((pd.read_csv(f, header = 0) for f in all_filenames))\n",
    "    df = df.drop_duplicates(subset = ['ClaimID', 'LineNumber'],keep = 'last').reset_index(drop = True)\n",
    "    df[\"PatientGender\"] = df[\"PatientGender.1\"]\n",
    "    df = df.drop(columns=['PatientGender.1'])\n",
    "    strBucket = 'data-science-synthdata-dev'\n",
    "    strTestFile =\"Professional/Month_\"+ str(i)+\".csv\"\n",
    "    strOutFile = 's3://{}/{}'.format(strBucket, strTestFile)\n",
    "    df.to_csv(strOutFile, sep=\"|\", index=False)\n",
    "    df.to_csv(\"Out/Professional/To_s3/\"+str(i)+\".csv\", index=False)\n",
    "\n",
    "fold_lst1=[\"Out/Professional/Anesthesiology_provider_file1.csv\", \"Out/Professional/Counseling_Psychology_provider_file1.csv\", \"Out/Professional/Emergency_Medicine_provider_file1.csv\", \"Out/Professional/Nuclear_Radiology_provider_file1.csv\", \"Out/Professional/Obstetrics_Gynecology_provider_file1.csv\", \"Out/Professional/Ophthalmology_provider_file1.csv\", \"Out/Professional/Orthopaedic_Surgery_provider_file1.csv\", \"Out/Professional/Pathology_provider_file1.csv\", \"Out/Professional/Psychiatry_Neurology_provider_file1.csv\", \"Out/Professional/Pediatrics_provider_file1.csv\"]\n",
    "combined_csv = pd.concat([pd.read_csv(f, header=0) for f in fold_lst1 ])\n",
    "in_lst =combined_csv.groupby(['ProviderNPI']).predicted_specialty_id_1.apply(lambda x: x.tolist()).reset_index().to_dict(orient='list')\n",
    "fin = []\n",
    "in_lst = in_lst[\"predicted_specialty_id_1\"]\n",
    "for s in in_lst:\n",
    "    s = s + [\"\"] * (3 - len(s))\n",
    "    fin.append(s)\n",
    "combined_csv[\"lst\"]=fin\n",
    "\n",
    "combined_csv[[\"predicted_specialty_id_1\",\"predicted_specialty_id_2\", \"predicted_specialty_id_3\"]] = pd.DataFrame(combined_csv.pop('lst').values.tolist(), index=combined_csv.index)\n",
    "combined_csv = combined_csv.drop_duplicates(subset = ['ProviderNPI'],keep = 'first').reset_index(drop = True)\n",
    "\n",
    "strOutFile = 's3://{}/{}'.format('data-science-synthdata-dev', \"Professional/predicted_claim_spec_synthetic.csv\")\n",
    "#combined_csv.to_csv(strOutFile, sep=\"|\", index=False)\n",
    "\n",
    "fold_lst2=[\"Out/Professional/Anesthesiology_provider_file2.csv\", \"Out/Professional/Counseling_Psychology_provider_file2.csv\", \"Out/Professional/Emergency_Medicine_provider_file2.csv\", \"Out/Professional/Nuclear_Radiology_provider_file2.csv\", \"Out/Professional/Obstetrics_Gynecology_provider_file2.csv\", \"Out/Professional/Ophthalmology_provider_file2.csv\", \"Out/Professional/Orthopaedic_Surgery_provider_file2.csv\", \"Out/Professional/Pathology_provider_file2.csv\", \"Out/Professional/Psychiatry_Neurology_provider_file2.csv\", \"Out/Professional/Pediatrics_provider_file2.csv\"]\n",
    "combined_csv_2 = pd.concat([pd.read_csv(f, header=0) for f in fold_lst2 ])\n",
    "combined_csv_2 = combined_csv_2.drop_duplicates()\n",
    "combined_csv = combined_csv.append(combined_csv_2)\n",
    "\n",
    "in_lst =combined_csv.groupby(['ProviderNPI']).predicted_specialty_id_1.apply(lambda x: x.tolist()).reset_index().to_dict(orient='list')\n",
    "fin = []\n",
    "in_lst = in_lst[\"predicted_specialty_id_1\"]\n",
    "for s in in_lst:\n",
    "    s = s + [\"\"] * (3 - len(s))\n",
    "    fin.append(s)\n",
    "combined_csv[\"lst\"]=fin\n",
    "\n",
    "combined_csv[[\"predicted_specialty_id_1\",\"predicted_specialty_id_2\", \"predicted_specialty_id_3\"]] = pd.DataFrame(combined_csv.pop('lst').values.tolist(), index=combined_csv.index)\n",
    "combined_csv = combined_csv.drop_duplicates(subset = ['ProviderNPI'],keep = 'first').reset_index(drop = True)\n",
    "\n",
    "\n",
    "strOutFile = 's3://{}/{}'.format('data-science-synthdata-dev', \"Professional/predicted_provider_spec_synthetic.csv\")\n",
    "combined_csv.to_csv(strOutFile, sep=\"|\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18411d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd8820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17597ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Input/rendering_prov.csv\")\n",
    "df[df[\"Tax\"].str.contains(\"counseling_psychology\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efc966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfce778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8762f10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0178d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe69b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_network(x):\n",
    "    if x ==\"PAR\":\n",
    "        return \"N\"\n",
    "    return \"Y\"\n",
    "ren_df = pd.read_csv(\"Input/ren_provider_overlap_cnts.csv\")\n",
    "ren_df = ren_df.sort_values(by=['perc'], ascending=False)\n",
    "ren_df = ren_df.drop(columns=[\"size\"])\n",
    "ren_dict =dict(ren_df.values)\n",
    "\n",
    "\n",
    "num_providers = 100000#may want to edit this\n",
    "rendering_df = pd.DataFrame()\n",
    "rendering_df[\"RenderingProviderEntityType\"] = np.random.choice([\"I\", \"O\"], num_providers, p=[0.9, 0.1])\n",
    "rendering_df['RenderingProviderPhoneNumber'] = [fake.msisdn() for i in range(num_providers)]\n",
    "rendering_df['RenderingProviderTIN']= np.random.randint(100000000,999999999, size=num_providers)\n",
    "rendering_df[\"RenderingProviderID\"] = np.random.randint(10000,999999, size=num_providers)\n",
    "rendering_df[\"RenderingProviderID\"] = rendering_df[\"RenderingProviderID\"].astype(str)\n",
    "rendering_df[\"ContractedFlag\"] = np.random.choice(['PAR','NONPAR'], num_providers, p=[0.8, 0.2])\n",
    "rendering_df[\"OutOfNetwork\"]  = rendering_df[\"ContractedFlag\"].apply(lambda x: in_network(x))\n",
    "rendering_df['1'] = [fake.building_number() for i in range(num_providers)]\n",
    "rendering_df[\"2\"] = [fake.street_name() for i in range(num_providers)]\n",
    "rendering_df['RenderingProviderAddressLine1'] = rendering_df[\"1\"]+\" \"+rendering_df[\"2\"]\n",
    "rendering_df[\"1\"] = np.random.choice([\"Suite\", \"Apt.\", \"Null\"], num_providers, p=[0.05, 0.15, 0.8])\n",
    "rendering_df[\"2\"] = [fake.building_number() for i in range(num_providers)]\n",
    "rendering_df[\"RenderingProviderAddressLine2\"] = rendering_df[\"1\"]+\" \"+rendering_df[\"2\"]\n",
    "rendering_df.loc[rendering_df['RenderingProviderAddressLine2'].str.contains('Null', case=False), 'RenderingProviderAddressLine2'] = ''\n",
    "\n",
    "\n",
    "org1_df = rendering_df.loc[(rendering_df['RenderingProviderEntityType'] == 'O')]\n",
    "ind1_df = rendering_df.loc[(rendering_df['RenderingProviderEntityType'] == 'I')]\n",
    "ind1_df['RenderingProviderOrg'] = \"\"\n",
    "ind1_df['RenderingProviderFName'] = [fake.first_name() for i in range(ind1_df.shape[0])]\n",
    "ind1_df['RenderingProviderMName'] = [fake.first_name() for i in range(ind1_df.shape[0])]\n",
    "ind1_df['RenderingProviderLName'] = [fake.last_name() for i in range(ind1_df.shape[0])]\n",
    "ind1_df['prob'] = np.random.randint(1, 3, size=ind1_df.shape[0])\n",
    "\n",
    "org1_df['RenderingProviderOrg'] = np.random.choice(org_lst, size=org1_df.shape[0])\n",
    "org1_df['RenderingProviderFName'] =\"\"\n",
    "org1_df['RenderingProviderMName'] =\"\"\n",
    "org1_df['RenderingProviderLName'] =\"\"\n",
    "org1_df['prob'] = np.random.randint(5, 15, size=org1_df.shape[0])\n",
    "org1_df = org1_df.drop_duplicates(subset = ['RenderingProviderOrg'],keep = 'last').reset_index(drop = True)\n",
    "\n",
    "rendering_df= pd.concat([org1_df, ind1_df])\n",
    "\n",
    "\n",
    "rendering_df['RenderingProviderPhoneNumber'] = rendering_df['RenderingProviderPhoneNumber'].astype(str)\n",
    "rendering_df['RenderingProviderTIN'] = rendering_df['RenderingProviderTIN'].astype(str)\n",
    "rendering_df[\"CCN\"] = np.random.randint(0,38000, size=rendering_df.shape[0])\n",
    "rendering_df[\"CCN\"] = rendering_df[\"CCN\"].apply(lambda x: '{0:0>5}'.format(x))\n",
    "rendering_df[\"RenderingProviderNPI\"] = np.random.randint(1000000000, 9999999999, size=rendering_df.shape[0])\n",
    "rendering_df[\"RenderingProviderNPI\"] = rendering_df[\"RenderingProviderNPI\"].astype(str)\n",
    "rendering_df['RenderingProviderTIN'] = rendering_df['RenderingProviderTIN'].astype(str)\n",
    "rendering_df[\"RenderingProviderID\"] = rendering_df[\"RenderingProviderID\"].astype(str)\n",
    "rendering_df[\"CCN\"] =rendering_df[\"CCN\"].astype(str)\n",
    "\n",
    "\n",
    "rendering_df[\"REN\"] =rendering_df[\"RenderingProviderEntityType\"]+\"@\"+rendering_df[\"RenderingProviderPhoneNumber\"]+\"@\"+rendering_df[\"RenderingProviderTIN\"]+\"@\"+rendering_df[\"RenderingProviderID\"]+\"@\"+rendering_df[\"ContractedFlag\"]+\"@\"+rendering_df[\"OutOfNetwork\"]+\"@\"+rendering_df[\"RenderingProviderAddressLine1\"]+\"@\"+rendering_df[\"RenderingProviderAddressLine2\"]+\"@\"+rendering_df[\"RenderingProviderOrg\"]+\"@\"+rendering_df[\"RenderingProviderFName\"]+\"@\"+rendering_df[\"RenderingProviderMName\"]+\"@\"+rendering_df[\"RenderingProviderLName\"]+\"@\"+rendering_df[\"CCN\"]+\"@\"+rendering_df[\"RenderingProviderNPI\"]\n",
    "rendering_df = rendering_df.drop(columns=[\"RenderingProviderEntityType\",\"RenderingProviderPhoneNumber\",\n",
    "                                          \"RenderingProviderTIN\",\"RenderingProviderID\",\"ContractedFlag\",\n",
    "                                          \"OutOfNetwork\",\"RenderingProviderAddressLine1\",\n",
    "                                          \"RenderingProviderAddressLine2\",\"RenderingProviderOrg\",\n",
    "                                          \"RenderingProviderFName\",\"RenderingProviderMName\",\"RenderingProviderLName\",\n",
    "                                          \"CCN\",\"RenderingProviderNPI\"])\n",
    "rendering_df[\"Tax\"] = np.random.choice(list(ren_dict.keys()),rendering_df.shape[0], p=list(ren_dict.values()))\n",
    "rendering_df = rendering_df.drop(columns=[\"1\", \"2\"])\n",
    " \n",
    "rendering_df = rendering_df[[\"REN\",'prob', \"Tax\"]]\n",
    "rendering_df.to_csv(\"Input/rendering_prov.csv\", index=False)\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "billing_df = pd.DataFrame()\n",
    "bill_df = pd.read_csv(\"Input/bill_provider_overlap_cnts.csv\")\n",
    "bill_df = bill_df.sort_values(by=['perc'], ascending=False)\n",
    "bill_df = bill_df.drop(columns=[\"size\"])\n",
    "bill_dict =dict(bill_df.values)\n",
    "\n",
    "\n",
    "billing_df[\"BillingProviderID\"] = np.random.randint(10000,999999, size=num_providers//6)\n",
    "billing_df[\"BillingProviderTIN\"] = np.random.randint(100000000,999999999, size=num_providers//6)\n",
    "billing_df['BillingProviderPhoneNumber'] = [fake.msisdn() for i in range(num_providers//6)]\n",
    "billing_df[\"BillingProviderEntityType\"] = np.random.choice([\"I\", \"O\"], num_providers//6, p=[0.9, 0.1])\n",
    "#split billing to deal with individual vs orginizations diffrently\n",
    "org_df = billing_df.loc[(billing_df['BillingProviderEntityType'] == 'O')]\n",
    "ind_df= billing_df.loc[(billing_df['BillingProviderEntityType'] == 'I')]\n",
    "\n",
    "ind_df['BillingProviderFName'] = [fake.first_name() for i in range(ind_df.shape[0])]\n",
    "ind_df['BillingProviderMName'] = [fake.first_name() for i in range(ind_df.shape[0])]\n",
    "ind_df['BillingProviderLName'] = [fake.last_name() for i in range(ind_df.shape[0])]\n",
    "ind_df[\"BillingProviderOrg\"]  = \"\"\n",
    "ind_df['prob'] = np.random.randint(1, 3, size=ind_df.shape[0])\n",
    "\n",
    "org_df[\"BillingProviderOrg\"] = np.random.choice(org_lst, size=org_df.shape[0])\n",
    "org_df['BillingProviderFName'] = \"\"\n",
    "org_df['BillingProviderMName'] = \"\"\n",
    "org_df['BillingProviderLName'] = \"\"\n",
    "org_df = org_df.drop_duplicates(subset = ['BillingProviderOrg'],keep = 'last').reset_index(drop = True)\n",
    "org_df['prob'] = np.random.randint(5, 15, size=org_df.shape[0])\n",
    "\n",
    "billing_df = pd.concat([org_df, ind_df])\n",
    "\n",
    "billing_df[\"BillingProviderNPI\"]  = np.random.randint(1000000000, 9999999999, size=billing_df.shape[0])\n",
    "billing_df['1'] = [fake.building_number() for i in range(billing_df.shape[0])]\n",
    "billing_df[\"2\"] = [fake.street_name() for i in range(billing_df.shape[0])]\n",
    "billing_df['BillingProviderAddressLine1'] = billing_df[\"1\"]+\" \"+billing_df[\"2\"]\n",
    "billing_df[\"1\"] = np.random.choice([\"Suite\", \"Apt.\", \"Null\"], billing_df.shape[0], p=[0.05, 0.15, 0.8])\n",
    "billing_df[\"2\"] = [fake.building_number() for i in range(billing_df.shape[0])]\n",
    "billing_df[\"BillingProviderAddressLine2\"] = billing_df[\"1\"]+\" \"+billing_df[\"2\"]\n",
    "billing_df.loc[billing_df['BillingProviderAddressLine2'].str.contains('Null', case=False), \n",
    "               'BillingProviderAddressLine2'] = ''\n",
    "billing_df = billing_df.drop(columns=['1',\"2\"])\n",
    "billing_df[\"BillingProviderNPI\"] = billing_df[\"BillingProviderNPI\"].astype(str)\n",
    "billing_df['BillingProviderID'] = billing_df['BillingProviderID'].astype(str)\n",
    "billing_df['BillingProviderTIN'] = billing_df['BillingProviderTIN'].astype(str)\n",
    "\n",
    "billing_df[\"BILL\"] = billing_df[\"BillingProviderID\"]+\"@\"+billing_df[\"BillingProviderTIN\"]+\"@\"+billing_df[\"BillingProviderOrg\"]+\"@\"+billing_df[\"BillingProviderFName\"]+\"@\"+billing_df[\"BillingProviderMName\"]+\"@\"+billing_df[\"BillingProviderLName\"]+\"@\"+billing_df[\"BillingProviderPhoneNumber\"]+\"@\"+billing_df[\"BillingProviderAddressLine1\"]+\"@\"+billing_df[\"BillingProviderAddressLine2\"]+\"@\"+billing_df[\"BillingProviderEntityType\"]+\"@\"+billing_df[\"BillingProviderNPI\"]\n",
    "billing_df = billing_df.drop(columns = [\"BillingProviderID\",\"BillingProviderTIN\",\"BillingProviderOrg\",\n",
    "                                        \"BillingProviderFName\",\"BillingProviderMName\",\"BillingProviderLName\",\n",
    "                                        \"BillingProviderPhoneNumber\",\"BillingProviderAddressLine1\",\n",
    "                                        \"BillingProviderAddressLine2\",\"BillingProviderEntityType\",\n",
    "                                        \"BillingProviderNPI\"])\n",
    "\n",
    "billing_df[\"Tax\"] = np.random.choice(list(bill_dict.keys()),billing_df.shape[0], p=list(bill_dict.values()))\n",
    "billing_df = billing_df[[\"BILL\",'prob', \"Tax\"]]\n",
    "billing_df.to_csv(\"Input/billing_prov.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e98bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "billing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rendering_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5037a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "i = 0\n",
    "while i < 23:\n",
    "    i+=1\n",
    "    print(i)\n",
    "    all_filenames = [\"Out/Professional/\"+str(i)+\"/Anesthesiology_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Counseling_Psychology_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Emergency_Medicine_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Nuclear_Radiology_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Obstetrics_Gynecology_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Ophthalmology_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Orthopaedic_Surgery_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Pathology_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Pediatrics_Month_\"+str(i)+\".csv\", \"Out/Professional/\"+str(i)+\"/Psychiatry_Neurology_Month_\"+str(i)+\".csv\"]\n",
    "    df = pd.concat((pd.read_csv(f, header = 0) for f in all_filenames))\n",
    "    df = df.drop_duplicates(subset = ['ClaimID', 'LineNumber'],keep = 'last').reset_index(drop = True)\n",
    "    strBucket = 'data-science-synthdata-dev'\n",
    "    strTestFile =\"Professional/Month_\"+ str(i)+\".csv\"\n",
    "    strOutFile = 's3://{}/{}'.format(strBucket, strTestFile)\n",
    "    df.to_csv(strOutFile, sep=\"|\", index=False)\n",
    "    df.to_csv(\"Out/Professional/To_s3/Prof\"+str(i)+\".csv\", index=False)\n",
    "\n",
    "fold_lst1=[\"Out/Professional/Anesthesiology_provider_file1.csv\", \"Out/Professional/Counseling_Psychology_provider_file1.csv\", \"Out/Professional/Emergency_Medicine_provider_file1.csv\", \"Out/Professional/Nuclear_Radiology_provider_file1.csv\", \"Out/Professional/Obstetrics_Gynecology_provider_file1.csv\", \"Out/Professional/Ophthalmology_provider_file1.csv\", \"Out/Professional/Orthopaedic_Surgery_provider_file1.csv\", \"Out/Professional/Pathology_provider_file1.csv\", \"Out/Professional/Psychiatry_Neurology_provider_file1.csv\", \"Out/Professional/Pediatrics_provider_file1.csv\"]\n",
    "fold_lst2=[\"Out/Professional/Anesthesiology_provider_file2.csv\", \"Out/Professional/Counseling_Psychology_provider_file2.csv\", \"Out/Professional/Emergency_Medicine_provider_file2.csv\", \"Out/Professional/Nuclear_Radiology_provider_file2.csv\", \"Out/Professional/Obstetrics_Gynecology_provider_file2.csv\", \"Out/Professional/Ophthalmology_provider_file2.csv\", \"Out/Professional/Orthopaedic_Surgery_provider_file2.csv\", \"Out/Professional/Pathology_provider_file2.csv\", \"Out/Professional/Psychiatry_Neurology_provider_file2.csv\", \"Out/Professional/Pediatrics_provider_file2.csv\"]\n",
    "combined_csv = pd.concat([pd.read_csv(f, header=0) for f in fold_lst1 ])\n",
    "combined_csv = combined_csv.drop_duplicates()\n",
    "strOutFile = 's3://{}/{}'.format('data-science-synthdata-dev', \"Professional/predicted_provider_spec_synthetic.csv\")\n",
    "\n",
    "combined_csv.to_csv(strOutFile, sep=\"|\", index=False)\n",
    "combined_csv = pd.concat([pd.read_csv(f, header=0) for f in fold_lst2 ])\n",
    "combined_csv = combined_csv.drop_duplicates()\n",
    "strOutFile = 's3://{}/{}'.format('data-science-synthdata-dev', \"Professional/predicted_claim_spec_synthetic.csv\")\n",
    "combined_csv.to_csv(strOutFile, sep=\"|\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653742d7",
   "metadata": {},
   "source": [
    "i = 23\n",
    "while i < 24:\n",
    "    i+=1\n",
    "    print(i)\n",
    "    strBucket = 'data-science-synthdata-dev'\n",
    "    strTestFile =\"Professional/tmp_outlier_provs/Month_\"+ str(i)+\".csv\"\n",
    "    strOutFile = 's3://{}/{}'.format(strBucket, strTestFile)\n",
    "    \n",
    "    final_df = pd.read_csv(strOutFile, sep=\"|\")\n",
    "    #final_df['paid']=final_df['paid'].where(final_df['paid'] <= 10000000, 10000000)\n",
    "    final_df[\"LineNumber\"]= final_df[\"LineNumber\"].astype(int)\n",
    "    final_df[\"AccidentDate\"]=final_df[\"AccidentDate\"].fillna(\"\")\n",
    "    final_df[\"HdrCharges\"]=final_df[\"HdrCharges\"].where(final_df['HdrCharges'] <= 10000000, 10000000)\n",
    "    final_df[\"LineCharges\"]=final_df[\"LineCharges\"].where(final_df['LineCharges'] <= 10000000, 10000000)\n",
    "    final_df[\"HdrPayerPay\"]=final_df[\"HdrPayerPay\"].where(final_df['HdrPayerPay'] <= 10000000, 10000000)\n",
    "    final_df[\"HdrCOBPay\"]=final_df[\"HdrCOBPay\"].where(final_df['HdrCOBPay'] <= 10000000, 10000000)\n",
    "    final_df[\"HdrPatientCopay\"]=final_df[\"HdrPatientCopay\"].where(final_df['HdrPatientCopay'] <= 10000000, 10000000)\n",
    "    final_df[\"HdrPatientPay\"]=final_df[\"HdrPatientPay\"].where(final_df['HdrPatientPay'] <= 10000000, 10000000)\n",
    "    final_df[\"HdrPay\"]=final_df[\"HdrPay\"].where(final_df['HdrPay'] <= 10000000, 10000000)\n",
    "    final_df[\"LinePatientPaid\"]=final_df[\"LinePatientPaid\"].where(final_df['LinePatientPaid'] <= 10000000, 10000000)\n",
    "    final_df[\"LineAmountCoinsurance\"]=final_df[\"LineAmountCoinsurance\"].where(final_df['LineAmountCoinsurance'] <= 10000000, 10000000)\n",
    "    final_df[\"LineAmountCopay\"]=final_df[\"LineAmountCopay\"].where(final_df['LineAmountCopay'] <= 10000000, 10000000)\n",
    "    final_df[\"LineAmountDeductible\"]=final_df[\"LineAmountDeductible\"].where(final_df['LineAmountDeductible'] <= 10000000, 10000000)\n",
    "    final_df[\"LineCharges\"]=final_df[\"LineCharges\"].where(final_df['LineCharges'] <= 10000000, 10000000)\n",
    "    final_df[\"LinePayerPaid\"]=final_df[\"LinePayerPaid\"].where(final_df['LinePayerPaid'] <= 10000000, 10000000)\n",
    "    final_df[\"LineCOBPay\"]=final_df[\"LineCOBPay\"].where(final_df['LineCOBPay'] <= 10000000, 10000000)\n",
    "    final_df[\"LineTotPaid\"]=final_df[\"LineTotPaid\"].where(final_df['LineTotPaid'] <= 10000000, 10000000)\n",
    "    final_df[\"LineConsideredAmount\"]=final_df[\"LineConsideredAmount\"].where(final_df['LineConsideredAmount'] <= 10000000, 10000000)\n",
    "    final_df[\"LineAllowedAmount\"]=final_df[\"LineAllowedAmount\"].where(final_df['LineAllowedAmount'] <= 10000000, 10000000)\n",
    "    \n",
    "    \n",
    "    strBucket = 'data-science-synthdata-dev'\n",
    "    strTestFile =\"Professional/Month_\"+ str(i)+\".csv\"\n",
    "    strOutFile = 's3://{}/{}'.format(strBucket, strTestFile)\n",
    "    final_df.to_csv(strOutFile, sep=\"|\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409bf2dc",
   "metadata": {},
   "source": [
    "fold_lst1=[\"Out/Professional/Anesthesiology_provider_file1.csv\", \"Out/Professional/Counseling_Psychology_provider_file1.csv\", \"Out/Professional/Emergency_Medicine_provider_file1.csv\", \"Out/Professional/Nuclear_Radiology_provider_file1.csv\", \"Out/Professional/Obstetrics_Gynecology_provider_file1.csv\", \"Out/Professional/Ophthalmology_provider_file1.csv\", \"Out/Professional/Orthopaedic_Surgery_provider_file1.csv\", \"Out/Professional/Pathology_provider_file1.csv\", \"Out/Professional/Psychiatry_Neurology_provider_file1.csv\", \"Out/Professional/Pediatrics_provider_file1.csv\"]\n",
    "\n",
    "combined_csv = pd.concat([pd.read_csv(f, header=0) for f in fold_lst1 ])\n",
    "combined_csv = combined_csv.drop_duplicates()\n",
    "\n",
    "\n",
    "strOutFile = 's3://{}/{}'.format('data-science-synthdata-dev', \"predicted_provider_spec_synthetic.csv\")\n",
    "\n",
    "combined_csv.to_csv(strOutFile, sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0ee74",
   "metadata": {},
   "source": [
    "print(\"!\")\n",
    "fold_lst1=[\"Out/Professional/Anesthesiology_provider_file1.csv\", \"Out/Professional/Counseling_Psychology_provider_file1.csv\", \"Out/Professional/Emergency_Medicine_provider_file1.csv\", \"Out/Professional/Nuclear_Radiology_provider_file1.csv\", \"Out/Professional/Obstetrics_Gynecology_provider_file1.csv\", \"Out/Professional/Ophthalmology_provider_file1.csv\", \"Out/Professional/Orthopaedic_Surgery_provider_file1.csv\", \"Out/Professional/Pathology_provider_file1.csv\", \"Out/Professional/Psychiatry_Neurology_provider_file1.csv\", \"Out/Professional/Pediatrics_provider_file1.csv\"]\n",
    "fold_lst2=[\"Out/Professional/Anesthesiology_provider_file2.csv\", \"Out/Professional/Counseling_Psychology_provider_file2.csv\", \"Out/Professional/Emergency_Medicine_provider_file2.csv\", \"Out/Professional/Nuclear_Radiology_provider_file2.csv\", \"Out/Professional/Obstetrics_Gynecology_provider_file2.csv\", \"Out/Professional/Ophthalmology_provider_file2.csv\", \"Out/Professional/Orthopaedic_Surgery_provider_file2.csv\", \"Out/Professional/Pathology_provider_file2.csv\", \"Out/Professional/Psychiatry_Neurology_provider_file2.csv\", \"Out/Professional/Pediatrics_provider_file2.csv\"]\n",
    "combined_csv = pd.concat([pd.read_csv(f, header=0) for f in fold_lst1 ])\n",
    "combined_csv = combined_csv.drop_duplicates()\n",
    "strOutFile = 's3://{}/{}'.format('data-science-synthdata-dev', \"Professional/predicted_provider_spec_synthetic.csv\")\n",
    "\n",
    "combined_csv.to_csv(strOutFile, sep=\"|\", index=False)\n",
    "combined_csv = pd.concat([pd.read_csv(f, header=0) for f in fold_lst2 ])\n",
    "combined_csv = combined_csv.drop_duplicates()\n",
    "strOutFile = 's3://{}/{}'.format('data-science-synthdata-dev', \"Professional/predicted_claim_spec_synthetic.csv\")\n",
    "combined_csv.to_csv(strOutFile, sep=\"|\", index=False)\n",
    "print(\"#\")\n",
    "all_filenames = [\"Out/Professional/Anesthesiology.csv\", \"Out/Professional/Counseling_Psychology.csv\", \"Out/Professional/Emergency_Medicine.csv\", \"Out/Professional/Nuclear_Radiology.csv\", \"Out/Professional/Obstetrics_Gynecology.csv\", \"Out/Professional/Ophthalmology.csv\", \"Out/Professional/Orthopaedic_Surgery.csv\", \"Out/Professional/Pathology.csv\", \"Out/Professional/Pediatrics.csv\", \"Out/Professional/Psychiatry_Neurology.csv\"]\n",
    "final_df = pd.concat((pd.read_csv(f, header = 0) for f in all_filenames))\n",
    "print(\"@\")\n",
    "final_df.to_csv(\"Out/To_s3/all.csv\", sep=\"|\", index=False)\n",
    "final_df = final_df.sort_values(by=['ClaimID'])\n",
    "final_df = final_df.sort_values(by=['ClaimPaidDate'])\n",
    "final_df[\"Date\"] = final_df[\"ClaimPaidDate\"]\n",
    "final_df[\"Date\"] = final_df[\"Date\"].astype(str)\n",
    "final_df[\"Date\"] = final_df[\"Date\"].str[:7]\n",
    "final_df = final_df.drop_duplicates()\n",
    "strBucket = 'data-science-synthdata-dev'\n",
    "out = final_df\n",
    "print(\"!\")\n",
    "out = out.drop(columns=['Date'])\n",
    "i = 0\n",
    "j = 0\n",
    "k = 0\n",
    "c = \"\"\n",
    "print(\"%\")\n",
    "for index, row in final_df.iterrows():\n",
    "    j+=1\n",
    "    if not (c ==row[\"Date\"]):\n",
    "        c = row[\"Date\"]\n",
    "        if k!=0:\n",
    "            print(i)\n",
    "            strBucket = 'data-science-synthdata-dev'\n",
    "            strTestFile = \"Professional/Month_\"+str(i)+'.csv'\n",
    "            strOutFile = 's3://{}/{}'.format(strBucket, strTestFile)\n",
    "            out.iloc[k:j-1].to_csv(strOutFile, sep=\"|\", index=False)\n",
    "        k = j\n",
    "        i+=1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
